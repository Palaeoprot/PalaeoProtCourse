{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SYzs_ZvCXBC2cbSYUEdEzxR5hqKgSzyF",
      "authorship_tag": "ABX9TyMc3HEqTr2wlrhlr3q95CO6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palaeoprot/PalaeoProtCourse/blob/main/Tartu_Excercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paleoproteomics Data Analysis Workflow\n",
        "\n",
        "This notebook is designed for the Paleoproteomics Course (March 17-21, 2025) to help you understand how to process and analyze mass spectrometry data from NovorCloud. Each section is explained to help you understand the purpose of the code and how it works.\n"
      ],
      "metadata": {
        "id": "ad2arFdPyeRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive\n",
        "This section mounts your Google Drive to access files stored there. This is necessary because we'll be working with data files from NovorCloud that are stored in Google Drive.\n"
      ],
      "metadata": {
        "id": "tSRyF97cdlYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (if using Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Jk3aakcocQP0",
        "outputId": "6f044a15-e9c5-4bca-f43a-f52a2232fd77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the Environment\n",
        "This section imports all the required libraries for our analysis. We'll use several Python libraries:\n",
        "- Standard libraries for file handling, regular expressions, and data structures\n",
        "- NumPy and Pandas for data manipulation\n",
        "- Matplotlib and Seaborn for data visualization"
      ],
      "metadata": {
        "id": "ccWCQh38dV4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "import glob\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "bVypfOd8dZJS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Extraction and Organization\n",
        "This section identifies and extracts data from zip files that were downloaded from NovorCloud. The code:\n",
        "1. Looks for all zip files in the specified directory\n",
        "2. Creates an output directory for extracted files if it doesn't exist\n",
        "3. Extracts two types of files: parameters.js (search settings) and .peps.txt (peptide identifications)\n",
        "4. Parses the filenames to extract metadata (student, sample, conditions)\n",
        "5. Creates dataframes to track the extracted files and their metadata\n",
        "6. Saves this metadata to CSV files for reference"
      ],
      "metadata": {
        "id": "mBFIDBaZdh3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "\n",
        "# Define paths\n",
        "zip_files_dir = '/content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/NovorCloud Output'  # Path to where your zip files are\n",
        "extracted_dir = '/content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files'  # Path where you want to extract files\n",
        "output_csv = '/content/combined_peptides_parameters.csv'\n",
        "\n",
        "# Create extraction directory if it doesn't exist\n",
        "os.makedirs(extracted_dir, exist_ok=True)\n",
        "\n",
        "# First, extract all the zip files\n",
        "zip_files = [f for f in os.listdir(zip_files_dir) if f.endswith('.zip')]\n",
        "print(f\"Found {len(zip_files)} zip files to extract\")\n",
        "\n",
        "for zip_file in zip_files:\n",
        "    zip_path = os.path.join(zip_files_dir, zip_file)\n",
        "    print(f\"Extracting: {zip_file}\")\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            # Create a folder for each zip file to keep organization\n",
        "            zip_extract_folder = os.path.join(extracted_dir, os.path.splitext(zip_file)[0])\n",
        "            os.makedirs(zip_extract_folder, exist_ok=True)\n",
        "\n",
        "            # Extract all files\n",
        "            zip_ref.extractall(zip_extract_folder)\n",
        "            print(f\"  Successfully extracted to {zip_extract_folder}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR extracting {zip_file}: {str(e)}\")\n",
        "\n",
        "# Now perform the recursive search for extracted files\n",
        "all_peps_files = []\n",
        "all_params_files = []\n",
        "\n",
        "# Walk through all subdirectories in the extracted directory\n",
        "for root, dirs, files in os.walk(extracted_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.peps.txt'):\n",
        "            all_peps_files.append(os.path.join(root, file))\n",
        "        elif file.endswith('parameters.js'):\n",
        "            all_params_files.append(os.path.join(root, file))\n",
        "\n",
        "print(f\"Found {len(all_peps_files)} peptide files and {len(all_params_files)} parameter files\")\n",
        "\n",
        "# If we didn't find any files, show directory contents for debugging\n",
        "if len(all_peps_files) == 0 and len(all_params_files) == 0:\n",
        "    print(\"\\nNo files found. Debugging directory structure:\")\n",
        "    all_files = []\n",
        "    for root, dirs, files in os.walk(extracted_dir):\n",
        "        rel_path = os.path.relpath(root, extracted_dir)\n",
        "        print(f\"\\nDirectory: {rel_path}\")\n",
        "        if files:\n",
        "            print(\"  Files:\")\n",
        "            for file in files:\n",
        "                print(f\"    - {file}\")\n",
        "                all_files.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"\\nTotal files found: {len(all_files)}\")\n",
        "\n",
        "    # Check if we need to look for different file extensions\n",
        "    if len(all_files) > 0:\n",
        "        extensions = {}\n",
        "        for file in all_files:\n",
        "            ext = os.path.splitext(file)[1]\n",
        "            if ext in extensions:\n",
        "                extensions[ext] += 1\n",
        "            else:\n",
        "                extensions[ext] = 1\n",
        "\n",
        "        print(\"\\nFile extensions found:\")\n",
        "        for ext, count in extensions.items():\n",
        "            print(f\"  {ext}: {count} files\")\n",
        "\n",
        "# Build dictionaries to map between peps files and parameter files\n",
        "param_mapping = {}\n",
        "\n",
        "# This structure is different from what we expected\n",
        "# The parameters.js file is in the root of each extracted zip folder\n",
        "# The peptide files are in subdirectories (F_1, F_2, F_3, etc.)\n",
        "\n",
        "for peps_file in all_peps_files:\n",
        "    peps_dirname = os.path.dirname(peps_file)\n",
        "    peps_basename = os.path.basename(peps_file)\n",
        "\n",
        "    # Go up one level to find the root of the extracted zip\n",
        "    # (from \"folder/F_1 - name\" to \"folder\")\n",
        "    zip_root = os.path.dirname(peps_dirname)\n",
        "\n",
        "    # Look for parameters.js in the zip root directory\n",
        "    param_file_path = os.path.join(zip_root, \"parameters.js\")\n",
        "\n",
        "    if os.path.exists(param_file_path):\n",
        "        param_mapping[peps_file] = param_file_path\n",
        "        print(f\"Matched (zip structure): {peps_basename} -> parameters.js\")\n",
        "    else:\n",
        "        print(f\"No parameter file found for: {peps_basename}\")\n",
        "\n",
        "# Debugging: Print out all mappings\n",
        "print(\"\\nAll parameter file mappings:\")\n",
        "for peps_file, param_file in param_mapping.items():\n",
        "    print(f\"{os.path.basename(peps_file)} -> {os.path.basename(param_file)}\")\n",
        "\n",
        "# Prepare list to hold DataFrames\n",
        "all_dfs = []\n",
        "\n",
        "# Process each peps.txt file\n",
        "for peps_path in all_peps_files:\n",
        "    peps_filename = os.path.basename(peps_path)\n",
        "    print(f\"\\nProcessing: {peps_filename}\")\n",
        "\n",
        "    # Get matching parameter file\n",
        "    param_path = param_mapping.get(peps_path)\n",
        "\n",
        "    if not param_path:\n",
        "        print(f\"  WARNING: No parameter file found for {peps_filename}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Read parameter file\n",
        "        with open(param_path, 'r') as f:\n",
        "            param_content = f.read()\n",
        "\n",
        "        # Extract JSON portion - specific pattern for NovorCloud files\n",
        "        match = re.search(r'const parameters = (.+)', param_content)\n",
        "        if match:\n",
        "            param_json_str = match.group(1)\n",
        "            parameters = json.loads(param_json_str)\n",
        "            print(f\"  Successfully parsed parameters from {os.path.basename(param_path)}\")\n",
        "        else:\n",
        "            print(f\"  ERROR: Could not extract parameters from {param_path}\")\n",
        "            continue\n",
        "\n",
        "        # Read peptide file\n",
        "        try:\n",
        "            peps_df = pd.read_csv(\n",
        "                peps_path,\n",
        "                comment='#',\n",
        "                skip_blank_lines=True,\n",
        "                skipinitialspace=True,\n",
        "                on_bad_lines='warn'\n",
        "            )\n",
        "\n",
        "            if peps_df.empty:\n",
        "                print(f\"  WARNING: Empty peptide file: {peps_filename}\")\n",
        "                continue\n",
        "\n",
        "            # Extract metadata from filename\n",
        "            filename_parts = peps_filename.split('_F_1')[0] if '_F_1' in peps_filename else peps_filename.split('.peps.txt')[0]\n",
        "\n",
        "            # Try to extract student/sample/conditions\n",
        "            if '-' in filename_parts:\n",
        "                parts = filename_parts.split('-')\n",
        "                peps_df['student'] = parts[0]\n",
        "                if len(parts) > 1:\n",
        "                    peps_df['sample'] = parts[1]\n",
        "                if len(parts) > 2:\n",
        "                    peps_df['conditions'] = parts[2]\n",
        "            else:\n",
        "                parts = filename_parts.split('_')\n",
        "                peps_df['student'] = parts[0]\n",
        "                if len(parts) > 1:\n",
        "                    peps_df['sample'] = '_'.join(parts[1:])\n",
        "\n",
        "            # Add parameter data\n",
        "            for key, value in parameters.items():\n",
        "                if isinstance(value, (list, dict)):\n",
        "                    peps_df[key] = json.dumps(value)\n",
        "                else:\n",
        "                    peps_df[key] = value\n",
        "\n",
        "            # Add reference columns\n",
        "            peps_df['peps_file'] = peps_filename\n",
        "            peps_df['param_file'] = os.path.basename(param_path)\n",
        "\n",
        "            # Add to list of dataframes\n",
        "            all_dfs.append(peps_df)\n",
        "            print(f\"  Added {len(peps_df)} rows from {peps_filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ERROR reading peptide file {peps_path}: {str(e)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR processing parameter file {param_path}: {str(e)}\")\n",
        "\n",
        "# Combine all dataframes\n",
        "if all_dfs:\n",
        "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "    combined_df.to_csv(output_csv, index=False)\n",
        "    print(f\"\\n✅ Combined DataFrame saved to: {output_csv}\")\n",
        "    print(f\"Total rows: {len(combined_df)}\")\n",
        "    print(f\"Total unique peptides: {combined_df['peptide'].nunique()}\")\n",
        "\n",
        "    # Basic statistics\n",
        "    if 'score' in combined_df.columns:\n",
        "        print(f\"Score statistics: min={combined_df['score'].min()}, max={combined_df['score'].max()}, mean={combined_df['score'].mean():.2f}\")\n",
        "else:\n",
        "    print(\"No peptide files processed.\")"
      ],
      "metadata": {
        "id": "6SvpM-Wgnlt4",
        "outputId": "a8a9433f-c576-4579-e694-631178c7e7af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 18 zip files to extract\n",
            "Extracting: IGK-turkey-bostaurusfasta.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/IGK-turkey-bostaurusfasta\n",
            "Extracting: MC-cow-cowfasta.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/MC-cow-cowfasta\n",
            "Extracting: RB_swissprot_mammoth.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/RB_swissprot_mammoth\n",
            "Extracting: KT_cow_cowfasta_(AutoEnzymes).zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/KT_cow_cowfasta_(AutoEnzymes)\n",
            "Extracting: SS_cow_cowfasta_AutoEnzymes.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/SS_cow_cowfasta_AutoEnzymes\n",
            "Extracting: LS_Edmontosaur_bos_taurus.fasta.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/LS_Edmontosaur_bos_taurus.fasta\n",
            "Extracting: MC-turkey-swissprot-default.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/MC-turkey-swissprot-default\n",
            "Extracting: HK-cow-cowfasta.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/HK-cow-cowfasta\n",
            "Extracting: MC-Turkey using PG2IG.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/MC-Turkey using PG2IG\n",
            "Extracting: MC-Turkey+Moa-collagens_only.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/MC-Turkey+Moa-collagens_only\n",
            "Extracting: LS_ Edmontosaur_nr_fasta.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/LS_ Edmontosaur_nr_fasta\n",
            "Extracting: MC-Moa-Phas.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/MC-Moa-Phas\n",
            "Extracting: SKK-Moa-Phas.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/SKK-Moa-Phas\n",
            "Extracting: PR_Turkey_Phas.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/PR_Turkey_Phas\n",
            "Extracting: PR_Mammoth_pel_Swissprot.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/PR_Mammoth_pel_Swissprot\n",
            "Extracting: RH-Bos.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/RH-Bos\n",
            "Extracting: MT-Mammoth-sup.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/MT-Mammoth-sup\n",
            "Extracting: HK_Mammoth-cow&elephant&phasnid-collagen+contamin.zip\n",
            "  Successfully extracted to /content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files/HK_Mammoth-cow&elephant&phasnid-collagen+contamin\n",
            "Found 27 peptide files and 25 parameter files\n",
            "No parameter file found for: IGK-turkey-bostaurusfasta_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "No parameter file found for: Pauline_Mammoth_Swissprot_F_1 - 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "No parameter file found for: MC-cow-cowfasta_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "No parameter file found for: RB_swissprot_mammoth_F_1 - 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "No parameter file found for: KT_cow_cowfasta_(AutoEnzymes)_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "No parameter file found for: SS_cow_cowfasta_AutoEnzymes_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "No parameter file found for: LS_Edmontosaur_bos_taurus_F_1 - 290423_Edmontosaur_1hr_Neat_290423_Edmontosaur_1hr_Neat.peps.txt\n",
            "Matched (zip structure): 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> parameters.js\n",
            "Matched (zip structure): 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> parameters.js\n",
            "Matched (zip structure): 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt -> parameters.js\n",
            "Matched (zip structure): 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> parameters.js\n",
            "Matched (zip structure): 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> parameters.js\n",
            "Matched (zip structure): 290423_Edmontosaur_1hr_Neat.peps.txt -> parameters.js\n",
            "Matched (zip structure): 260423_Turkey_Collagen_1hr_1_in_100.peps.txt -> parameters.js\n",
            "Matched (zip structure): 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> parameters.js\n",
            "Matched (zip structure): 260423_Turkey_Collagen_1hr_1_in_100.peps.txt -> parameters.js\n",
            "Matched (zip structure): 260423_Turkey_Collagen_1hr_1_in_100.peps.txt -> parameters.js\n",
            "Matched (zip structure): ERS_Moa_N400_MOA3_20Mar2018_24.peps.txt -> parameters.js\n",
            "Matched (zip structure): ERS_Moa_400_MOA1_20Mar2018_26.peps.txt -> parameters.js\n",
            "Matched (zip structure): 290423_Edmontosaur_1hr_Neat.peps.txt -> parameters.js\n",
            "Matched (zip structure): ERS_Moa_N400_MOA3_20Mar2018_24.peps.txt -> parameters.js\n",
            "Matched (zip structure): ERS_Moa_N400_MOA3_20Mar2018_24.peps.txt -> parameters.js\n",
            "Matched (zip structure): 260423_Turkey_Collagen_1hr_1_in_100.peps.txt -> parameters.js\n",
            "Matched (zip structure): 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt -> parameters.js\n",
            "Matched (zip structure): BOS-Tendon-Powdered-Trypsin-HCD.peps.txt -> parameters.js\n",
            "Matched (zip structure): 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt -> parameters.js\n",
            "Matched (zip structure): 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_pel_01.peps.txt -> parameters.js\n",
            "\n",
            "All parameter file mappings:\n",
            "260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> parameters.js\n",
            "260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> parameters.js\n",
            "20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt -> parameters.js\n",
            "260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> parameters.js\n",
            "260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> parameters.js\n",
            "290423_Edmontosaur_1hr_Neat.peps.txt -> parameters.js\n",
            "260423_Turkey_Collagen_1hr_1_in_100.peps.txt -> parameters.js\n",
            "260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> parameters.js\n",
            "260423_Turkey_Collagen_1hr_1_in_100.peps.txt -> parameters.js\n",
            "260423_Turkey_Collagen_1hr_1_in_100.peps.txt -> parameters.js\n",
            "ERS_Moa_N400_MOA3_20Mar2018_24.peps.txt -> parameters.js\n",
            "ERS_Moa_400_MOA1_20Mar2018_26.peps.txt -> parameters.js\n",
            "290423_Edmontosaur_1hr_Neat.peps.txt -> parameters.js\n",
            "ERS_Moa_N400_MOA3_20Mar2018_24.peps.txt -> parameters.js\n",
            "ERS_Moa_N400_MOA3_20Mar2018_24.peps.txt -> parameters.js\n",
            "260423_Turkey_Collagen_1hr_1_in_100.peps.txt -> parameters.js\n",
            "20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt -> parameters.js\n",
            "BOS-Tendon-Powdered-Trypsin-HCD.peps.txt -> parameters.js\n",
            "20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt -> parameters.js\n",
            "20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_pel_01.peps.txt -> parameters.js\n",
            "\n",
            "Processing: IGK-turkey-bostaurusfasta_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  WARNING: No parameter file found for IGK-turkey-bostaurusfasta_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: Pauline_Mammoth_Swissprot_F_1 - 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "  WARNING: No parameter file found for Pauline_Mammoth_Swissprot_F_1 - 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "\n",
            "Processing: MC-cow-cowfasta_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  WARNING: No parameter file found for MC-cow-cowfasta_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: RB_swissprot_mammoth_F_1 - 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "  WARNING: No parameter file found for RB_swissprot_mammoth_F_1 - 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "\n",
            "Processing: KT_cow_cowfasta_(AutoEnzymes)_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  WARNING: No parameter file found for KT_cow_cowfasta_(AutoEnzymes)_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: SS_cow_cowfasta_AutoEnzymes_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  WARNING: No parameter file found for SS_cow_cowfasta_AutoEnzymes_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: LS_Edmontosaur_bos_taurus_F_1 - 290423_Edmontosaur_1hr_Neat_290423_Edmontosaur_1hr_Neat.peps.txt\n",
            "  WARNING: No parameter file found for LS_Edmontosaur_bos_taurus_F_1 - 290423_Edmontosaur_1hr_Neat_290423_Edmontosaur_1hr_Neat.peps.txt\n",
            "\n",
            "Processing: 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 6110 rows from 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 6142 rows from 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 25148 rows from 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "\n",
            "Processing: 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 6117 rows from 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 6131 rows from 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: 290423_Edmontosaur_1hr_Neat.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 5962 rows from 290423_Edmontosaur_1hr_Neat.peps.txt\n",
            "\n",
            "Processing: 260423_Turkey_Collagen_1hr_1_in_100.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 8177 rows from 260423_Turkey_Collagen_1hr_1_in_100.peps.txt\n",
            "\n",
            "Processing: 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 6120 rows from 260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: 260423_Turkey_Collagen_1hr_1_in_100.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 7898 rows from 260423_Turkey_Collagen_1hr_1_in_100.peps.txt\n",
            "\n",
            "Processing: 260423_Turkey_Collagen_1hr_1_in_100.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 7684 rows from 260423_Turkey_Collagen_1hr_1_in_100.peps.txt\n",
            "\n",
            "Processing: ERS_Moa_N400_MOA3_20Mar2018_24.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 12373 rows from ERS_Moa_N400_MOA3_20Mar2018_24.peps.txt\n",
            "\n",
            "Processing: ERS_Moa_400_MOA1_20Mar2018_26.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 12173 rows from ERS_Moa_400_MOA1_20Mar2018_26.peps.txt\n",
            "\n",
            "Processing: 290423_Edmontosaur_1hr_Neat.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 6533 rows from 290423_Edmontosaur_1hr_Neat.peps.txt\n",
            "\n",
            "Processing: ERS_Moa_N400_MOA3_20Mar2018_24.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 12407 rows from ERS_Moa_N400_MOA3_20Mar2018_24.peps.txt\n",
            "\n",
            "Processing: ERS_Moa_N400_MOA3_20Mar2018_24.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 12407 rows from ERS_Moa_N400_MOA3_20Mar2018_24.peps.txt\n",
            "\n",
            "Processing: 260423_Turkey_Collagen_1hr_1_in_100.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 7879 rows from 260423_Turkey_Collagen_1hr_1_in_100.peps.txt\n",
            "\n",
            "Processing: 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 25309 rows from 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "\n",
            "Processing: BOS-Tendon-Powdered-Trypsin-HCD.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 6289 rows from BOS-Tendon-Powdered-Trypsin-HCD.peps.txt\n",
            "\n",
            "Processing: 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 24666 rows from 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "\n",
            "Processing: 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_pel_01.peps.txt\n",
            "  Successfully parsed parameters from parameters.js\n",
            "  Added 24700 rows from 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_pel_01.peps.txt\n",
            "\n",
            "✅ Combined DataFrame saved to: /content/combined_peptides_parameters.csv\n",
            "Total rows: 230225\n",
            "Total unique peptides: 160301\n",
            "Score statistics: min=-3.5036, max=33.975, mean=1.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enriching the Data\n",
        "This section loads the combined data file and enhances it by processing the various parameters from the search. The code:\n",
        "1. Loads the previously combined peptide and parameter data\n",
        "2. Processes parameter columns that contain complex data (like lists of PTMs)\n",
        "3. Creates boolean columns for each type of parameter (e.g., each enzyme type, each fixed or variable PTM)\n",
        "4. Extracts raw filename and database filename information\n",
        "5. Parses actual sample types based on raw filenames\n",
        "6. Provides detailed information about the updated dataframe\n",
        "7. Saves the enriched dataframe to a new CSV file\n",
        "\n",
        "\n",
        "This enrichment makes the data easier to analyze because it transforms complex nested data into simple boolean columns.\n",
        "\n"
      ],
      "metadata": {
        "id": "vEFurbyMy_Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the combined data\n",
        "file_path = '/content/combined_peptides_parameters.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(\"Initial DataFrame shape (rows, columns):\", df.shape)\n",
        "\n",
        "# Process parameter columns that are currently stored as strings but represent lists or objects\n",
        "params_to_process = [\n",
        "    'spectraFilenames',\n",
        "    'dbFilenames',\n",
        "    'fixedPtms',\n",
        "    'variablePtms'\n",
        "]\n",
        "\n",
        "# Extract values from string representations of lists\n",
        "for param in params_to_process:\n",
        "    if param in df.columns:\n",
        "        # Create new columns for each item in these lists\n",
        "        try:\n",
        "            # First, try to parse the string representations of lists\n",
        "            new_values = {}\n",
        "\n",
        "            for idx, value in df[param].items():\n",
        "                if pd.isna(value):\n",
        "                    continue\n",
        "\n",
        "                # Handle string representations of lists\n",
        "                if isinstance(value, str):\n",
        "                    # Clean up the string to make it properly parseable\n",
        "                    clean_value = value.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
        "\n",
        "                    try:\n",
        "                        # Try to parse as JSON\n",
        "                        parsed = json.loads(clean_value)\n",
        "                    except:\n",
        "                        try:\n",
        "                            # Try to parse as Python literal\n",
        "                            parsed = ast.literal_eval(value)\n",
        "                        except:\n",
        "                            # If all parsing fails, just use the string as is\n",
        "                            parsed = value\n",
        "\n",
        "                    new_values[idx] = parsed\n",
        "                else:\n",
        "                    new_values[idx] = value\n",
        "\n",
        "            # Create a new series with the parsed values\n",
        "            parsed_series = pd.Series(new_values)\n",
        "\n",
        "            # For list columns, create individual columns for each item\n",
        "            if len(parsed_series) > 0 and isinstance(parsed_series.iloc[0], list):\n",
        "                # Get all unique values across all lists\n",
        "                all_items = set()\n",
        "                for idx, items in parsed_series.items():\n",
        "                    if isinstance(items, list):\n",
        "                        all_items.update(items)\n",
        "\n",
        "                # Create a column for each item\n",
        "                for item in sorted(all_items):\n",
        "                    col_name = f\"{param}_{item.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "                    df[col_name] = False\n",
        "\n",
        "                    # Mark True where the item exists in the list\n",
        "                    for idx, items in parsed_series.items():\n",
        "                        if isinstance(items, list) and item in items:\n",
        "                            df.at[idx, col_name] = True\n",
        "\n",
        "            # For scalar values, just keep the original column\n",
        "            else:\n",
        "                df[param] = parsed_series\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {param}: {e}\")\n",
        "\n",
        "# Extract raw filename from spectraFilenames\n",
        "if 'spectraFilenames' in df.columns:\n",
        "    def get_first_raw_file(value):\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if isinstance(value, list) and len(value) > 0:\n",
        "                return value[0]\n",
        "            elif isinstance(value, str):\n",
        "                # Try to parse as a list if it looks like one\n",
        "                if value.startswith('[') and value.endswith(']'):\n",
        "                    try:\n",
        "                        parsed = json.loads(value.replace(\"'\", '\"'))\n",
        "                        if isinstance(parsed, list) and len(parsed) > 0:\n",
        "                            return parsed[0]\n",
        "                    except:\n",
        "                        pass\n",
        "                return value\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    df['raw_filename'] = df['spectraFilenames'].apply(get_first_raw_file)\n",
        "\n",
        "    raw_file_cols = [col for col in df.columns if col.startswith('spectraFilenames_') and col != 'spectraFilenames']\n",
        "    if len(raw_file_cols) > 0 and df['raw_filename'].isna().any():\n",
        "        print(f\"Using fallback method to extract raw filenames. Found {len(raw_file_cols)} file columns.\")\n",
        "\n",
        "        def extract_from_boolean_cols(row):\n",
        "            if pd.notna(row['raw_filename']):\n",
        "                return row['raw_filename']\n",
        "\n",
        "            for col in raw_file_cols:\n",
        "                if row[col]:\n",
        "                    return col.replace('spectraFilenames_', '')\n",
        "\n",
        "            return None\n",
        "\n",
        "        df['raw_filename'] = df.apply(extract_from_boolean_cols, axis=1)\n",
        "        print(f\"After fallback, {df['raw_filename'].isna().sum()} rows still have no raw filename.\")\n",
        "\n",
        "\n",
        "# Extract database filename from dbFilenames\n",
        "if 'dbFilenames' in df.columns:\n",
        "    def get_first_db_file(value):\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if isinstance(value, list) and len(value) > 0:\n",
        "                return value[0]\n",
        "            elif isinstance(value, str):\n",
        "                # Try to parse as a list if it looks like one\n",
        "                if value.startswith('[') and value.endswith(']'):\n",
        "                    parsed = ast.literal_eval(value)\n",
        "                    if isinstance(parsed, list) and len(parsed) > 0:\n",
        "                        return parsed[0]\n",
        "                return value\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    df['db_filename'] = df['dbFilenames'].apply(get_first_db_file)\n",
        "\n",
        "# Extract enzyme information\n",
        "if 'enzymeSelection' in df.columns:\n",
        "    # Create boolean columns for each enzyme type\n",
        "    unique_enzymes = df['enzymeSelection'].unique()\n",
        "    for enzyme in unique_enzymes:\n",
        "        if pd.notna(enzyme):\n",
        "            col_name = f\"enzyme_{enzyme}\"\n",
        "            df[col_name] = df['enzymeSelection'] == enzyme\n",
        "\n",
        "# Extract PTM information into separate columns\n",
        "def create_ptm_columns(row):\n",
        "    \"\"\"Create boolean columns for each PTM type\"\"\"\n",
        "    result = {}\n",
        "\n",
        "    # Process fixed PTMs\n",
        "    if 'fixedPtms' in df.columns:\n",
        "        fixed_ptms = row['fixedPtms']\n",
        "        if isinstance(fixed_ptms, list):\n",
        "            for ptm in fixed_ptms:\n",
        "                result[f\"fixed_ptm_{ptm.replace(' ', '_')}\"] = True\n",
        "\n",
        "    # Process variable PTMs\n",
        "    if 'variablePtms' in df.columns:\n",
        "        var_ptms = row['variablePtms']\n",
        "        if isinstance(var_ptms, list):\n",
        "            for ptm in var_ptms:\n",
        "                result[f\"var_ptm_{ptm.replace(' ', '_')}\"] = True\n",
        "\n",
        "    return result\n",
        "\n",
        "# Apply the function and add the columns\n",
        "if 'fixedPtms' in df.columns or 'variablePtms' in df.columns:\n",
        "    # First create all possible PTM columns\n",
        "    all_fixed_ptms = set()\n",
        "    all_var_ptms = set()\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Fixed PTMs\n",
        "        if 'fixedPtms' in df.columns:\n",
        "            fixed_ptms = row['fixedPtms']\n",
        "            if isinstance(fixed_ptms, list):\n",
        "                all_fixed_ptms.update(fixed_ptms)\n",
        "\n",
        "        # Variable PTMs\n",
        "        if 'variablePtms' in df.columns:\n",
        "            var_ptms = row['variablePtms']\n",
        "            if isinstance(var_ptms, list):\n",
        "                all_var_ptms.update(var_ptms)\n",
        "\n",
        "    # Create columns for all PTMs\n",
        "    for ptm in all_fixed_ptms:\n",
        "        col_name = f\"fixed_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "        df[col_name] = False\n",
        "\n",
        "    for ptm in all_var_ptms:\n",
        "        col_name = f\"var_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "        df[col_name] = False\n",
        "\n",
        "    # Now fill in the values\n",
        "    for idx, row in df.iterrows():\n",
        "        # Fixed PTMs\n",
        "        if 'fixedPtms' in df.columns:\n",
        "            fixed_ptms = row['fixedPtms']\n",
        "            if isinstance(fixed_ptms, list):\n",
        "                for ptm in fixed_ptms:\n",
        "                    col_name = f\"fixed_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "                    df.at[idx, col_name] = True\n",
        "\n",
        "        # Variable PTMs\n",
        "        if 'variablePtms' in df.columns:\n",
        "            var_ptms = row['variablePtms']\n",
        "            if isinstance(var_ptms, list):\n",
        "                for ptm in var_ptms:\n",
        "                    col_name = f\"var_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "                    df.at[idx, col_name] = True\n",
        "\n",
        "# Extract sample from raw filename\n",
        "def extract_sample_from_raw(raw_filename):\n",
        "    if pd.isna(raw_filename):\n",
        "        return \"unknown\"\n",
        "\n",
        "    raw_str = str(raw_filename).lower()\n",
        "\n",
        "    if 'bovine' in raw_str or 'cow' in raw_str:\n",
        "        return \"cow\"\n",
        "    elif 'mammoth' in raw_str:\n",
        "        return \"mammoth\"\n",
        "    elif 'edmontosaur' in raw_str:\n",
        "        return \"edmontosaurus\"\n",
        "    elif 'turkey' in raw_str:\n",
        "        return \"turkey\"\n",
        "    elif 'moa' in raw_str:\n",
        "        return \"moa\"\n",
        "    else:\n",
        "        return raw_str.split('.')[0]\n",
        "\n",
        "# Add actual_sample column based on raw filename\n",
        "if 'raw_filename' in df.columns:\n",
        "    df['actual_sample'] = df['raw_filename'].apply(extract_sample_from_raw)\n",
        "\n",
        "# Print information about the updated dataframe\n",
        "print(\"\\nUpdated DataFrame shape (rows, columns):\", df.shape)\n",
        "print(\"\\nNew columns added:\")\n",
        "new_cols = set(df.columns) - set(['line', 'target/decoy', 'nDecoy', 'specId', 'scanNum', 'mz', 'z', 'ppm', 'score',\n",
        "                             'peptide', 'protein', 'student', 'sample', 'conditions', 'original_filename', 'notes',\n",
        "                             'jobTitle', 'spectraFilenames', 'dbFilenames', 'fixedPtms', 'variablePtms',\n",
        "                             'enzymeSelection', 'fragmentationSelection', 'massAnalyzer', 'precursorErrorTol',\n",
        "                             'precursorErrorTolUnit', 'fragmentErrorTol', 'fragmentErrorTolUnit',\n",
        "                             'activateBuiltInPtms', 'peps_file', 'param_file', 'condition'])\n",
        "for col in sorted(new_cols):\n",
        "    print(f\"  {col}\")\n",
        "\n",
        "# Show sample distribution\n",
        "if 'actual_sample' in df.columns:\n",
        "    print(\"\\nSample distribution:\")\n",
        "    sample_counts = df['actual_sample'].value_counts()\n",
        "    for sample, count in sample_counts.items():\n",
        "        print(f\"  {sample}: {count} entries\")\n",
        "\n",
        "# Find unique combinations of samples and methods\n",
        "if 'actual_sample' in df.columns:\n",
        "    # Use database file as a proxy for search strategy\n",
        "    if 'db_filename' in df.columns:\n",
        "        print(\"\\nUnique sample-database combinations:\")\n",
        "        combinations = df.groupby(['actual_sample', 'db_filename']).size().reset_index(name='count')\n",
        "        for _, row in combinations.iterrows():\n",
        "            print(f\"  {row['actual_sample']} with {row['db_filename']}: {row['count']} entries\")\n",
        "\n",
        "        # Find samples analyzed with multiple databases\n",
        "        sample_db_counts = df.groupby('actual_sample')['db_filename'].nunique()\n",
        "        samples_with_multiple_dbs = sample_db_counts[sample_db_counts > 1].index.tolist()\n",
        "\n",
        "        print(f\"\\nSamples analyzed with multiple databases: {samples_with_multiple_dbs}\")\n",
        "\n",
        "        # For each such sample, show the databases\n",
        "        for sample in samples_with_multiple_dbs:\n",
        "            dbs = sorted(df[df['actual_sample'] == sample]['db_filename'].unique())\n",
        "            print(f\"  {sample}:\")\n",
        "            for db in dbs:\n",
        "                count = len(df[(df['actual_sample'] == sample) & (df['db_filename'] == db)])\n",
        "                print(f\"    {db}: {count} entries\")\n",
        "\n",
        "        # Now, for each sample with multiple databases, compare the spectrum identifications\n",
        "        for sample in samples_with_multiple_dbs:\n",
        "            print(f\"\\n===== COMPARING DATABASES FOR {sample.upper()} =====\")\n",
        "\n",
        "            # Get data for this sample\n",
        "            sample_data = df[df['actual_sample'] == sample]\n",
        "\n",
        "            # Get unique databases for this sample\n",
        "            dbs = sorted(sample_data['db_filename'].unique())\n",
        "\n",
        "            # Get spectra identified under each database\n",
        "            db_spectra = {}\n",
        "            for db in dbs:\n",
        "                db_df = sample_data[sample_data['db_filename'] == db]\n",
        "                spectra = set(db_df['specId'].astype(str))\n",
        "                db_spectra[db] = spectra\n",
        "                print(f\"  {db}: {len(spectra)} spectra identified\")\n",
        "\n",
        "            # Compare first two databases (for simplicity)\n",
        "            if len(dbs) >= 2:\n",
        "                db1, db2 = dbs[0], dbs[1]\n",
        "\n",
        "                # Calculate overlap\n",
        "                overlap = db_spectra[db1].intersection(db_spectra[db2])\n",
        "                only_db1 = db_spectra[db1] - db_spectra[db2]\n",
        "                only_db2 = db_spectra[db2] - db_spectra[db1]\n",
        "\n",
        "                # Calculate overlap percentages\n",
        "                overlap_pct_of_db1 = len(overlap) / len(db_spectra[db1]) * 100 if db_spectra[db1] else 0\n",
        "                overlap_pct_of_db2 = len(overlap) / len(db_spectra[db2]) * 100 if db_spectra[db2] else 0\n",
        "\n",
        "                print(f\"\\n  {db1} vs {db2}:\")\n",
        "                print(f\"    Overlap: {len(overlap)} spectra ({overlap_pct_of_db1:.1f}% of {db1}, {overlap_pct_of_db2:.1f}% of {db2})\")\n",
        "                print(f\"    Only in {db1}: {len(only_db1)} spectra\")\n",
        "                print(f\"    Only in {db2}: {len(only_db2)} spectra\")\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df.to_csv('/content/enriched_peps_parameters.csv', index=False)\n",
        "print(\"\\nEnriched DataFrame saved to: /content/enriched_peps_parameters.csv\")\n"
      ],
      "metadata": {
        "id": "htbhElkoztUs",
        "outputId": "5e6519e7-4df3-42e8-ab32-448ba131d01a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-4dbe9814cadb>:3: DtypeWarning: Columns (11,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial DataFrame shape (rows, columns): (230225, 30)\n",
            "\n",
            "Updated DataFrame shape (rows, columns): (230225, 72)\n",
            "\n",
            "New columns added:\n",
            "  actual_sample\n",
            "  dbFilenames_PhasStrut.fasta\n",
            "  dbFilenames_PhasStrutContam.fasta\n",
            "  dbFilenames_bos_taurus.fasta\n",
            "  dbFilenames_cow&elephant&phasnid-collagen+contamin.fasta\n",
            "  dbFilenames_cow&elephant&phasnid-collagen-PG2IG+contamin.fasta\n",
            "  dbFilenames_nr.fasta\n",
            "  dbFilenames_uniprot_sprot.fasta\n",
            "  db_filename\n",
            "  enzyme_Auto\n",
            "  enzyme_Trypsin\n",
            "  fixedPtms_Carbamidomethyl_C\n",
            "  raw_filename\n",
            "  spectraFilenames_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_pel_01.raw\n",
            "  spectraFilenames_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.raw\n",
            "  spectraFilenames_260423_Bovine_Collagen_1hr_1_in_1000.raw\n",
            "  spectraFilenames_260423_Turkey_Collagen_1hr_1_in_100.raw\n",
            "  spectraFilenames_290423_Edmontosaur_1hr_Neat.raw\n",
            "  spectraFilenames_BOS-Tendon-Powdered-Trypsin-HCD.raw\n",
            "  spectraFilenames_ERS_Moa_400_MOA1_20Mar2018_26.raw\n",
            "  spectraFilenames_ERS_Moa_N400_MOA3_20Mar2018_24.raw\n",
            "  variablePtms_Carbamidomethyl_C\n",
            "  variablePtms_Carbamidomethyl_DHKE\n",
            "  variablePtms_Carbamidomethyl_N-term\n",
            "  variablePtms_Deamidated_N\n",
            "  variablePtms_Deamidated_NQ\n",
            "  variablePtms_Gal-HydK\n",
            "  variablePtms_Galactosyl-Hydroxylysine_K\n",
            "  variablePtms_Hydroxproline\n",
            "  variablePtms_HydroxyProline\n",
            "  variablePtms_Hydroxyproline\n",
            "  variablePtms_In_Source_Fragmentation_-H2O\n",
            "  variablePtms_Iron_C-term\n",
            "  variablePtms_Iron_DE\n",
            "  variablePtms_Oxidation_FHMW\n",
            "  variablePtms_Oxidation_M\n",
            "  variablePtms_Phospho_ST\n",
            "  variablePtms_ProlineOxydation\n",
            "  variablePtms_Pyro-Glu_E\n",
            "  variablePtms_Pyro-Glu_Q\n",
            "  variablePtms_deHydoxyLysine\n",
            "  variablePtms_hydroxyproline\n",
            "\n",
            "Sample distribution:\n",
            "  mammoth: 99823 entries\n",
            "  moa: 57044 entries\n",
            "  cow: 30620 entries\n",
            "  turkey: 23954 entries\n",
            "  edmontosaurus: 12495 entries\n",
            "  bos-tendon-powdered-trypsin-hcd: 6289 entries\n",
            "\n",
            "Unique sample-database combinations:\n",
            "\n",
            "Samples analyzed with multiple databases: []\n",
            "\n",
            "Enriched DataFrame saved to: /content/enriched_peps_parameters.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "This section provides a comprehensive overview of the dataframe structure and contents. The code:\n",
        "1. Shows the overall shape of the dataframe (number of rows and columns)\n",
        "2. Lists all column names with their indices for easy reference\n",
        "3. Displays the data types of each column\n",
        "4. Shows the first 5 rows of the dataframe\n",
        "\n",
        "This helps you understand what data you're working with and how it's structured before you begin your analysis."
      ],
      "metadata": {
        "id": "_KuTe2SMz3Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about the dataframe\n",
        "print(\"DataFrame shape (rows, columns):\", df.shape)\n",
        "\n",
        "# Display column names\n",
        "print(\"\\nColumn names:\")\n",
        "for i, col in enumerate(df.columns):\n",
        "    print(f\"{i+1}. {col}\")\n",
        "\n",
        "# Display data types of each column\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# First 5 rows\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "tmicDsIJ0AnP",
        "outputId": "4cc3710c-87a8-42bd-e2c0-726e156019a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame shape (rows, columns): (230225, 72)\n",
            "\n",
            "Column names:\n",
            "1. line\n",
            "2. target/decoy\n",
            "3. nDecoy\n",
            "4. specId\n",
            "5. scanNum\n",
            "6. mz\n",
            "7. z\n",
            "8. ppm\n",
            "9. score\n",
            "10. peptide\n",
            "11. protein\n",
            "12. student\n",
            "13. sample\n",
            "14. notes\n",
            "15. jobTitle\n",
            "16. spectraFilenames\n",
            "17. dbFilenames\n",
            "18. fixedPtms\n",
            "19. variablePtms\n",
            "20. enzymeSelection\n",
            "21. fragmentationSelection\n",
            "22. massAnalyzer\n",
            "23. precursorErrorTol\n",
            "24. precursorErrorTolUnit\n",
            "25. fragmentErrorTol\n",
            "26. fragmentErrorTolUnit\n",
            "27. activateBuiltInPtms\n",
            "28. peps_file\n",
            "29. param_file\n",
            "30. conditions\n",
            "31. spectraFilenames_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_pel_01.raw\n",
            "32. spectraFilenames_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.raw\n",
            "33. spectraFilenames_260423_Bovine_Collagen_1hr_1_in_1000.raw\n",
            "34. spectraFilenames_260423_Turkey_Collagen_1hr_1_in_100.raw\n",
            "35. spectraFilenames_290423_Edmontosaur_1hr_Neat.raw\n",
            "36. spectraFilenames_BOS-Tendon-Powdered-Trypsin-HCD.raw\n",
            "37. spectraFilenames_ERS_Moa_400_MOA1_20Mar2018_26.raw\n",
            "38. spectraFilenames_ERS_Moa_N400_MOA3_20Mar2018_24.raw\n",
            "39. dbFilenames_PhasStrut.fasta\n",
            "40. dbFilenames_PhasStrutContam.fasta\n",
            "41. dbFilenames_bos_taurus.fasta\n",
            "42. dbFilenames_cow&elephant&phasnid-collagen+contamin.fasta\n",
            "43. dbFilenames_cow&elephant&phasnid-collagen-PG2IG+contamin.fasta\n",
            "44. dbFilenames_nr.fasta\n",
            "45. dbFilenames_uniprot_sprot.fasta\n",
            "46. fixedPtms_Carbamidomethyl_C\n",
            "47. variablePtms_Carbamidomethyl_C\n",
            "48. variablePtms_Carbamidomethyl_DHKE\n",
            "49. variablePtms_Carbamidomethyl_N-term\n",
            "50. variablePtms_Deamidated_N\n",
            "51. variablePtms_Deamidated_NQ\n",
            "52. variablePtms_Gal-HydK\n",
            "53. variablePtms_Galactosyl-Hydroxylysine_K\n",
            "54. variablePtms_Hydroxproline\n",
            "55. variablePtms_HydroxyProline\n",
            "56. variablePtms_Hydroxyproline\n",
            "57. variablePtms_In_Source_Fragmentation_-H2O\n",
            "58. variablePtms_Iron_C-term\n",
            "59. variablePtms_Iron_DE\n",
            "60. variablePtms_Oxidation_FHMW\n",
            "61. variablePtms_Oxidation_M\n",
            "62. variablePtms_Phospho_ST\n",
            "63. variablePtms_ProlineOxydation\n",
            "64. variablePtms_Pyro-Glu_E\n",
            "65. variablePtms_Pyro-Glu_Q\n",
            "66. variablePtms_deHydoxyLysine\n",
            "67. variablePtms_hydroxyproline\n",
            "68. raw_filename\n",
            "69. db_filename\n",
            "70. enzyme_Trypsin\n",
            "71. enzyme_Auto\n",
            "72. actual_sample\n",
            "\n",
            "Data types:\n",
            "line               int64\n",
            "target/decoy      object\n",
            "nDecoy             int64\n",
            "specId             int64\n",
            "scanNum            int64\n",
            "                   ...  \n",
            "raw_filename      object\n",
            "db_filename       object\n",
            "enzyme_Trypsin      bool\n",
            "enzyme_Auto         bool\n",
            "actual_sample     object\n",
            "Length: 72, dtype: object\n",
            "\n",
            "First 5 rows:\n",
            "   line target/decoy  nDecoy  specId  scanNum        mz  z  ppm    score  \\\n",
            "0     1       target       0    2804     5860  1345.972  3  2.0  33.4171   \n",
            "1     2       target       0    6591     6230  1009.986  4  0.4  29.7917   \n",
            "2     3       target       0    1816     3966  1196.566  3 -3.2  25.2497   \n",
            "3     4       target       0    2824     4006  1210.582  3 -2.8  23.6676   \n",
            "4     5       target       0    6018     5476  1427.203  2 -4.6  22.8971   \n",
            "\n",
            "                                             peptide  ...  \\\n",
            "0  GENGPVGPTGPVGAAGPSGPNGPP(Hyp)GPAGSRGDGGPP(Hyp)...  ...   \n",
            "1  GFP(Hyp)GLP(Hyp)GPSGEPGKQGPSGASGERGPP(Hyp)GPMG...  ...   \n",
            "2  GNSGEPGAP(Hyp)GSKGDTGAKGEPGPTGIQGPP(Hyp)GPAGEEGKR  ...   \n",
            "3  GLVGEP(Hyp)GPAGSKGESGNKGEPGAVGQP(Hyp)GPP(Hyp)G...  ...   \n",
            "4        GLTGPIGPP(Hyp)GPAGAP(Hyp)GDKGEAGPSGPAGPTGAR  ...   \n",
            "\n",
            "  variablePtms_ProlineOxydation variablePtms_Pyro-Glu_E  \\\n",
            "0                         False                    True   \n",
            "1                         False                    True   \n",
            "2                         False                    True   \n",
            "3                         False                    True   \n",
            "4                         False                    True   \n",
            "\n",
            "  variablePtms_Pyro-Glu_Q  variablePtms_deHydoxyLysine  \\\n",
            "0                    True                        False   \n",
            "1                    True                        False   \n",
            "2                    True                        False   \n",
            "3                    True                        False   \n",
            "4                    True                        False   \n",
            "\n",
            "  variablePtms_hydroxyproline                              raw_filename  \\\n",
            "0                       False  260423_Bovine_Collagen_1hr_1_in_1000.raw   \n",
            "1                       False  260423_Bovine_Collagen_1hr_1_in_1000.raw   \n",
            "2                       False  260423_Bovine_Collagen_1hr_1_in_1000.raw   \n",
            "3                       False  260423_Bovine_Collagen_1hr_1_in_1000.raw   \n",
            "4                       False  260423_Bovine_Collagen_1hr_1_in_1000.raw   \n",
            "\n",
            "  db_filename enzyme_Trypsin enzyme_Auto actual_sample  \n",
            "0        None           True       False           cow  \n",
            "1        None           True       False           cow  \n",
            "2        None           True       False           cow  \n",
            "3        None           True       False           cow  \n",
            "4        None           True       False           cow  \n",
            "\n",
            "[5 rows x 72 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps for Analysis\n",
        "Now that the data is prepared and explored, you can add additional analysis cells to:\n",
        "\n",
        "1. **Filter peptides by sample type**:"
      ],
      "metadata": {
        "id": "0YvQ05s80Ib3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select which sample to analyze\n",
        "sample_to_analyze = \"moa\" #@param [\"mammoth\", \"moa\", \"cow\", \"turkey\", \"edmontosaurus\", \"all\"]\n",
        "\n",
        "# Filter dataframe to show only peptides from the selected sample\n",
        "if sample_to_analyze == \"all\":\n",
        "    selected_peptides = df  # Keep all samples\n",
        "    print(f\"Showing all {len(selected_peptides)} peptides\")\n",
        "else:\n",
        "    selected_peptides = df[df['actual_sample'] == sample_to_analyze]\n",
        "    print(f\"Found {len(selected_peptides)} {sample_to_analyze} peptides\")"
      ],
      "metadata": {
        "id": "mP2BfXEwtA2-",
        "outputId": "28ed2df9-dd12-4159-8ae1-f49d252e88e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 57044 moa peptides\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TfH3nYxQ0Te5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Analyze peptide modifications**:"
      ],
      "metadata": {
        "id": "bTyepnak0Qqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Count peptides with hydroxyproline modifications\n",
        "hyp_cols = [col for col in df.columns if 'hydroxy' in col.lower() and col.startswith('variable')]\n",
        "has_hyp = df[hyp_cols].any(axis=1)\n",
        "print(f\"Found {has_hyp.sum()} peptides with hydroxyproline modifications\")"
      ],
      "metadata": {
        "id": "2M-x4CYD0Xsr",
        "outputId": "7909b620-8d1a-4fd7-ef4f-56347ec7db05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 190790 peptides with hydroxyproline modifications\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Compare identification efficiency across samples**:"
      ],
      "metadata": {
        "id": "VhWQeD940iuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compare how many unique peptides were identified per sample\n",
        "peptides_by_sample = df.groupby('actual_sample')['peptide'].nunique()\n",
        "print(\"Unique peptides by sample:\")\n",
        "print(peptides_by_sample)"
      ],
      "metadata": {
        "id": "j1w1_s500nen",
        "outputId": "c12c56e5-9d6c-4a14-e7ef-b49d66360162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique peptides by sample:\n",
            "actual_sample\n",
            "bos-tendon-powdered-trypsin-hcd     4528\n",
            "cow                                19017\n",
            "edmontosaurus                      10746\n",
            "mammoth                            74473\n",
            "moa                                36911\n",
            "turkey                             17361\n",
            "Name: peptide, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Visualize score distributions**"
      ],
      "metadata": {
        "id": "KW8UymGF0r4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Create a histogram of peptide scores by sample\n",
        "plt.figure(figsize=(10, 6))\n",
        "for sample in df['actual_sample'].unique():\n",
        "  sample_data = df[df['actual_sample'] == sample]\n",
        "  plt.hist(sample_data['score'], alpha=0.5, label=sample, bins=20)\n",
        "plt.legend()\n",
        "plt.title('Distribution of Peptide Scores by Sample')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l58kSyYazbGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}