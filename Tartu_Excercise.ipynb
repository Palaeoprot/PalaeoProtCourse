{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SYzs_ZvCXBC2cbSYUEdEzxR5hqKgSzyF",
      "authorship_tag": "ABX9TyP3zWq5SznXoawu8TWmf6qF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palaeoprot/PalaeoProtCourse/blob/main/Tartu_Excercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paleoproteomics Data Analysis Workflow\n",
        "\n",
        "This notebook is designed for the Paleoproteomics Course (March 17-21, 2025) to help you understand how to process and analyze mass spectrometry data from NovorCloud. Each section is explained to help you understand the purpose of the code and how it works.\n"
      ],
      "metadata": {
        "id": "ad2arFdPyeRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive\n",
        "This section mounts your Google Drive to access files stored there. This is necessary because we'll be working with data files from NovorCloud that are stored in Google Drive.\n"
      ],
      "metadata": {
        "id": "tSRyF97cdlYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (if using Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Jk3aakcocQP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the Environment\n",
        "This section imports all the required libraries for our analysis. We'll use several Python libraries:\n",
        "- Standard libraries for file handling, regular expressions, and data structures\n",
        "- NumPy and Pandas for data manipulation\n",
        "- Matplotlib and Seaborn for data visualization"
      ],
      "metadata": {
        "id": "ccWCQh38dV4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "import glob\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "bVypfOd8dZJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Extraction and Organization\n",
        "This section identifies and extracts data from zip files that were downloaded from NovorCloud. The code:\n",
        "1. Looks for all zip files in the specified directory\n",
        "2. Creates an output directory for extracted files if it doesn't exist\n",
        "3. Extracts two types of files: parameters.js (search settings) and .peps.txt (peptide identifications)\n",
        "4. Parses the filenames to extract metadata (student, sample, conditions)\n",
        "5. Creates dataframes to track the extracted files and their metadata\n",
        "6. Saves this metadata to CSV files for reference"
      ],
      "metadata": {
        "id": "mBFIDBaZdh3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Define paths\n",
        "extracted_dir = '/content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files'\n",
        "output_csv = '/content/combined_peptides_parameters.csv'\n",
        "\n",
        "# Get lists of extracted files\n",
        "all_peps_files = glob.glob(os.path.join(extracted_dir, '*.peps.txt'))\n",
        "all_params_files = glob.glob(os.path.join(extracted_dir, '*_parameters.js'))  # Changed to .js extension\n",
        "\n",
        "print(f\"Found {len(all_peps_files)} peptide files and {len(all_params_files)} parameter files\")\n",
        "\n",
        "# Build dictionaries to map between peps files and parameter files\n",
        "param_mapping = {}\n",
        "\n",
        "# Create a mapping between peptide files and parameter files based on filename patterns\n",
        "for peps_file in all_peps_files:\n",
        "    peps_basename = os.path.basename(peps_file)\n",
        "\n",
        "    # Extract the prefix part (e.g., \"IGK-turkey-bostaurusfasta\")\n",
        "    prefix = peps_basename.split('_F_1')[0] if '_F_1' in peps_basename else peps_basename.split('.peps.txt')[0]\n",
        "\n",
        "    # Look for a matching parameter file\n",
        "    matching_param_file = None\n",
        "    param_filename = f\"{prefix}_parameters.js\"\n",
        "\n",
        "    for param_file in all_params_files:\n",
        "        if os.path.basename(param_file) == param_filename:\n",
        "            matching_param_file = param_file\n",
        "            break\n",
        "\n",
        "    if matching_param_file:\n",
        "        param_mapping[peps_file] = matching_param_file\n",
        "        print(f\"Matched: {os.path.basename(peps_file)} -> {os.path.basename(matching_param_file)}\")\n",
        "    else:\n",
        "        print(f\"No match found for: {os.path.basename(peps_file)}\")\n",
        "\n",
        "# Prepare list to hold DataFrames\n",
        "all_dfs = []\n",
        "\n",
        "# Process each peps.txt file\n",
        "for peps_path in all_peps_files:\n",
        "    peps_filename = os.path.basename(peps_path)\n",
        "    print(f\"\\nProcessing: {peps_filename}\")\n",
        "\n",
        "    # Get matching parameter file\n",
        "    param_path = param_mapping.get(peps_path)\n",
        "\n",
        "    if not param_path:\n",
        "        print(f\"  WARNING: No parameter file found for {peps_filename}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Read parameter file\n",
        "        with open(param_path, 'r') as f:\n",
        "            param_content = f.read()\n",
        "\n",
        "        # Extract JSON portion - specific pattern for your files\n",
        "        match = re.search(r'const parameters = (.+)', param_content)\n",
        "        if match:\n",
        "            param_json_str = match.group(1)\n",
        "            parameters = json.loads(param_json_str)\n",
        "            print(f\"  Successfully parsed parameters from {os.path.basename(param_path)}\")\n",
        "        else:\n",
        "            print(f\"  ERROR: Could not extract parameters from {param_path}\")\n",
        "            continue\n",
        "\n",
        "        # Read peptide file\n",
        "        try:\n",
        "            peps_df = pd.read_csv(\n",
        "                peps_path,\n",
        "                comment='#',\n",
        "                skip_blank_lines=True,\n",
        "                skipinitialspace=True,\n",
        "                on_bad_lines='warn'\n",
        "            )\n",
        "\n",
        "            if peps_df.empty:\n",
        "                print(f\"  WARNING: Empty peptide file: {peps_filename}\")\n",
        "                continue\n",
        "\n",
        "            # Extract metadata from filename\n",
        "            filename_parts = peps_filename.split('_F_1')[0]\n",
        "\n",
        "            # Try to extract student/sample/conditions\n",
        "            if '-' in filename_parts:\n",
        "                parts = filename_parts.split('-')\n",
        "                peps_df['student'] = parts[0]\n",
        "                if len(parts) > 1:\n",
        "                    peps_df['sample'] = parts[1]\n",
        "                if len(parts) > 2:\n",
        "                    peps_df['conditions'] = parts[2]\n",
        "            else:\n",
        "                parts = filename_parts.split('_')\n",
        "                peps_df['student'] = parts[0]\n",
        "                if len(parts) > 1:\n",
        "                    peps_df['sample'] = '_'.join(parts[1:])\n",
        "\n",
        "            # Add parameter data\n",
        "            for key, value in parameters.items():\n",
        "                if isinstance(value, (list, dict)):\n",
        "                    peps_df[key] = json.dumps(value)\n",
        "                else:\n",
        "                    peps_df[key] = value\n",
        "\n",
        "            # Add reference columns\n",
        "            peps_df['peps_file'] = peps_filename\n",
        "            peps_df['param_file'] = os.path.basename(param_path)\n",
        "\n",
        "            # Add to list of dataframes\n",
        "            all_dfs.append(peps_df)\n",
        "            print(f\"  Added {len(peps_df)} rows from {peps_filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ERROR reading peptide file {peps_path}: {str(e)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR processing parameter file {param_path}: {str(e)}\")\n",
        "\n",
        "# Combine all dataframes\n",
        "if all_dfs:\n",
        "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "    combined_df.to_csv(output_csv, index=False)\n",
        "    print(f\"\\n✅ Combined DataFrame saved to: {output_csv}\")\n",
        "    print(f\"Total rows: {len(combined_df)}\")\n",
        "    print(f\"Total unique peptides: {combined_df['peptide'].nunique()}\")\n",
        "\n",
        "    # Basic statistics\n",
        "    if 'score' in combined_df.columns:\n",
        "        print(f\"Score statistics: min={combined_df['score'].min()}, max={combined_df['score'].max()}, mean={combined_df['score'].mean():.2f}\")\n",
        "else:\n",
        "    print(\"No peptide files processed.\")"
      ],
      "metadata": {
        "id": "h8T3C6meA0Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enriching the Data\n",
        "This section loads the combined data file and enhances it by processing the various parameters from the search. The code:\n",
        "1. Loads the previously combined peptide and parameter data\n",
        "2. Processes parameter columns that contain complex data (like lists of PTMs)\n",
        "3. Creates boolean columns for each type of parameter (e.g., each enzyme type, each fixed or variable PTM)\n",
        "4. Extracts raw filename and database filename information\n",
        "5. Parses actual sample types based on raw filenames\n",
        "6. Provides detailed information about the updated dataframe\n",
        "7. Saves the enriched dataframe to a new CSV file\n",
        "\n",
        "\n",
        "This enrichment makes the data easier to analyze because it transforms complex nested data into simple boolean columns.\n",
        "\n"
      ],
      "metadata": {
        "id": "vEFurbyMy_Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the combined data\n",
        "file_path = '/content/combined_peptides_parameters.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(\"Initial DataFrame shape (rows, columns):\", df.shape)\n",
        "\n",
        "# Process parameter columns that are currently stored as strings but represent lists or objects\n",
        "params_to_process = [\n",
        "    'spectraFilenames',\n",
        "    'dbFilenames',\n",
        "    'fixedPtms',\n",
        "    'variablePtms'\n",
        "]\n",
        "\n",
        "# Extract values from string representations of lists\n",
        "for param in params_to_process:\n",
        "    if param in df.columns:\n",
        "        # Create new columns for each item in these lists\n",
        "        try:\n",
        "            # First, try to parse the string representations of lists\n",
        "            new_values = {}\n",
        "\n",
        "            for idx, value in df[param].items():\n",
        "                if pd.isna(value):\n",
        "                    continue\n",
        "\n",
        "                # Handle string representations of lists\n",
        "                if isinstance(value, str):\n",
        "                    # Clean up the string to make it properly parseable\n",
        "                    clean_value = value.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
        "\n",
        "                    try:\n",
        "                        # Try to parse as JSON\n",
        "                        parsed = json.loads(clean_value)\n",
        "                    except:\n",
        "                        try:\n",
        "                            # Try to parse as Python literal\n",
        "                            parsed = ast.literal_eval(value)\n",
        "                        except:\n",
        "                            # If all parsing fails, just use the string as is\n",
        "                            parsed = value\n",
        "\n",
        "                    new_values[idx] = parsed\n",
        "                else:\n",
        "                    new_values[idx] = value\n",
        "\n",
        "            # Create a new series with the parsed values\n",
        "            parsed_series = pd.Series(new_values)\n",
        "\n",
        "            # For list columns, create individual columns for each item\n",
        "            if len(parsed_series) > 0 and isinstance(parsed_series.iloc[0], list):\n",
        "                # Get all unique values across all lists\n",
        "                all_items = set()\n",
        "                for idx, items in parsed_series.items():\n",
        "                    if isinstance(items, list):\n",
        "                        all_items.update(items)\n",
        "\n",
        "                # Create a column for each item\n",
        "                for item in sorted(all_items):\n",
        "                    col_name = f\"{param}_{item.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "                    df[col_name] = False\n",
        "\n",
        "                    # Mark True where the item exists in the list\n",
        "                    for idx, items in parsed_series.items():\n",
        "                        if isinstance(items, list) and item in items:\n",
        "                            df.at[idx, col_name] = True\n",
        "\n",
        "            # For scalar values, just keep the original column\n",
        "            else:\n",
        "                df[param] = parsed_series\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {param}: {e}\")\n",
        "\n",
        "# Extract raw filename from spectraFilenames\n",
        "if 'spectraFilenames' in df.columns:\n",
        "    def get_first_raw_file(value):\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if isinstance(value, list) and len(value) > 0:\n",
        "                return value[0]\n",
        "            elif isinstance(value, str):\n",
        "                # Try to parse as a list if it looks like one\n",
        "                if value.startswith('[') and value.endswith(']'):\n",
        "                    try:\n",
        "                        parsed = json.loads(value.replace(\"'\", '\"'))\n",
        "                        if isinstance(parsed, list) and len(parsed) > 0:\n",
        "                            return parsed[0]\n",
        "                    except:\n",
        "                        pass\n",
        "                return value\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    df['raw_filename'] = df['spectraFilenames'].apply(get_first_raw_file)\n",
        "\n",
        "    raw_file_cols = [col for col in df.columns if col.startswith('spectraFilenames_') and col != 'spectraFilenames']\n",
        "    if len(raw_file_cols) > 0 and df['raw_filename'].isna().any():\n",
        "        print(f\"Using fallback method to extract raw filenames. Found {len(raw_file_cols)} file columns.\")\n",
        "\n",
        "        def extract_from_boolean_cols(row):\n",
        "            if pd.notna(row['raw_filename']):\n",
        "                return row['raw_filename']\n",
        "\n",
        "            for col in raw_file_cols:\n",
        "                if row[col]:\n",
        "                    return col.replace('spectraFilenames_', '')\n",
        "\n",
        "            return None\n",
        "\n",
        "        df['raw_filename'] = df.apply(extract_from_boolean_cols, axis=1)\n",
        "        print(f\"After fallback, {df['raw_filename'].isna().sum()} rows still have no raw filename.\")\n",
        "\n",
        "\n",
        "# Extract database filename from dbFilenames\n",
        "if 'dbFilenames' in df.columns:\n",
        "    def get_first_db_file(value):\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if isinstance(value, list) and len(value) > 0:\n",
        "                return value[0]\n",
        "            elif isinstance(value, str):\n",
        "                # Try to parse as a list if it looks like one\n",
        "                if value.startswith('[') and value.endswith(']'):\n",
        "                    parsed = ast.literal_eval(value)\n",
        "                    if isinstance(parsed, list) and len(parsed) > 0:\n",
        "                        return parsed[0]\n",
        "                return value\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    df['db_filename'] = df['dbFilenames'].apply(get_first_db_file)\n",
        "\n",
        "# Extract enzyme information\n",
        "if 'enzymeSelection' in df.columns:\n",
        "    # Create boolean columns for each enzyme type\n",
        "    unique_enzymes = df['enzymeSelection'].unique()\n",
        "    for enzyme in unique_enzymes:\n",
        "        if pd.notna(enzyme):\n",
        "            col_name = f\"enzyme_{enzyme}\"\n",
        "            df[col_name] = df['enzymeSelection'] == enzyme\n",
        "\n",
        "# Extract PTM information into separate columns\n",
        "def create_ptm_columns(row):\n",
        "    \"\"\"Create boolean columns for each PTM type\"\"\"\n",
        "    result = {}\n",
        "\n",
        "    # Process fixed PTMs\n",
        "    if 'fixedPtms' in df.columns:\n",
        "        fixed_ptms = row['fixedPtms']\n",
        "        if isinstance(fixed_ptms, list):\n",
        "            for ptm in fixed_ptms:\n",
        "                result[f\"fixed_ptm_{ptm.replace(' ', '_')}\"] = True\n",
        "\n",
        "    # Process variable PTMs\n",
        "    if 'variablePtms' in df.columns:\n",
        "        var_ptms = row['variablePtms']\n",
        "        if isinstance(var_ptms, list):\n",
        "            for ptm in var_ptms:\n",
        "                result[f\"var_ptm_{ptm.replace(' ', '_')}\"] = True\n",
        "\n",
        "    return result\n",
        "\n",
        "# Apply the function and add the columns\n",
        "if 'fixedPtms' in df.columns or 'variablePtms' in df.columns:\n",
        "    # First create all possible PTM columns\n",
        "    all_fixed_ptms = set()\n",
        "    all_var_ptms = set()\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Fixed PTMs\n",
        "        if 'fixedPtms' in df.columns:\n",
        "            fixed_ptms = row['fixedPtms']\n",
        "            if isinstance(fixed_ptms, list):\n",
        "                all_fixed_ptms.update(fixed_ptms)\n",
        "\n",
        "        # Variable PTMs\n",
        "        if 'variablePtms' in df.columns:\n",
        "            var_ptms = row['variablePtms']\n",
        "            if isinstance(var_ptms, list):\n",
        "                all_var_ptms.update(var_ptms)\n",
        "\n",
        "    # Create columns for all PTMs\n",
        "    for ptm in all_fixed_ptms:\n",
        "        col_name = f\"fixed_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "        df[col_name] = False\n",
        "\n",
        "    for ptm in all_var_ptms:\n",
        "        col_name = f\"var_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "        df[col_name] = False\n",
        "\n",
        "    # Now fill in the values\n",
        "    for idx, row in df.iterrows():\n",
        "        # Fixed PTMs\n",
        "        if 'fixedPtms' in df.columns:\n",
        "            fixed_ptms = row['fixedPtms']\n",
        "            if isinstance(fixed_ptms, list):\n",
        "                for ptm in fixed_ptms:\n",
        "                    col_name = f\"fixed_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "                    df.at[idx, col_name] = True\n",
        "\n",
        "        # Variable PTMs\n",
        "        if 'variablePtms' in df.columns:\n",
        "            var_ptms = row['variablePtms']\n",
        "            if isinstance(var_ptms, list):\n",
        "                for ptm in var_ptms:\n",
        "                    col_name = f\"var_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "                    df.at[idx, col_name] = True\n",
        "\n",
        "# Extract sample from raw filename\n",
        "def extract_sample_from_raw(raw_filename):\n",
        "    if pd.isna(raw_filename):\n",
        "        return \"unknown\"\n",
        "\n",
        "    raw_str = str(raw_filename).lower()\n",
        "\n",
        "    if 'bovine' in raw_str or 'cow' in raw_str:\n",
        "        return \"cow\"\n",
        "    elif 'mammoth' in raw_str:\n",
        "        return \"mammoth\"\n",
        "    elif 'edmontosaur' in raw_str:\n",
        "        return \"edmontosaurus\"\n",
        "    elif 'turkey' in raw_str:\n",
        "        return \"turkey\"\n",
        "    else:\n",
        "        return raw_str.split('.')[0]\n",
        "\n",
        "# Add actual_sample column based on raw filename\n",
        "if 'raw_filename' in df.columns:\n",
        "    df['actual_sample'] = df['raw_filename'].apply(extract_sample_from_raw)\n",
        "\n",
        "# Print information about the updated dataframe\n",
        "print(\"\\nUpdated DataFrame shape (rows, columns):\", df.shape)\n",
        "print(\"\\nNew columns added:\")\n",
        "new_cols = set(df.columns) - set(['line', 'target/decoy', 'nDecoy', 'specId', 'scanNum', 'mz', 'z', 'ppm', 'score',\n",
        "                             'peptide', 'protein', 'student', 'sample', 'conditions', 'original_filename', 'notes',\n",
        "                             'jobTitle', 'spectraFilenames', 'dbFilenames', 'fixedPtms', 'variablePtms',\n",
        "                             'enzymeSelection', 'fragmentationSelection', 'massAnalyzer', 'precursorErrorTol',\n",
        "                             'precursorErrorTolUnit', 'fragmentErrorTol', 'fragmentErrorTolUnit',\n",
        "                             'activateBuiltInPtms', 'peps_file', 'param_file', 'condition'])\n",
        "for col in sorted(new_cols):\n",
        "    print(f\"  {col}\")\n",
        "\n",
        "# Show sample distribution\n",
        "if 'actual_sample' in df.columns:\n",
        "    print(\"\\nSample distribution:\")\n",
        "    sample_counts = df['actual_sample'].value_counts()\n",
        "    for sample, count in sample_counts.items():\n",
        "        print(f\"  {sample}: {count} entries\")\n",
        "\n",
        "# Find unique combinations of samples and methods\n",
        "if 'actual_sample' in df.columns:\n",
        "    # Use database file as a proxy for search strategy\n",
        "    if 'db_filename' in df.columns:\n",
        "        print(\"\\nUnique sample-database combinations:\")\n",
        "        combinations = df.groupby(['actual_sample', 'db_filename']).size().reset_index(name='count')\n",
        "        for _, row in combinations.iterrows():\n",
        "            print(f\"  {row['actual_sample']} with {row['db_filename']}: {row['count']} entries\")\n",
        "\n",
        "        # Find samples analyzed with multiple databases\n",
        "        sample_db_counts = df.groupby('actual_sample')['db_filename'].nunique()\n",
        "        samples_with_multiple_dbs = sample_db_counts[sample_db_counts > 1].index.tolist()\n",
        "\n",
        "        print(f\"\\nSamples analyzed with multiple databases: {samples_with_multiple_dbs}\")\n",
        "\n",
        "        # For each such sample, show the databases\n",
        "        for sample in samples_with_multiple_dbs:\n",
        "            dbs = sorted(df[df['actual_sample'] == sample]['db_filename'].unique())\n",
        "            print(f\"  {sample}:\")\n",
        "            for db in dbs:\n",
        "                count = len(df[(df['actual_sample'] == sample) & (df['db_filename'] == db)])\n",
        "                print(f\"    {db}: {count} entries\")\n",
        "\n",
        "        # Now, for each sample with multiple databases, compare the spectrum identifications\n",
        "        for sample in samples_with_multiple_dbs:\n",
        "            print(f\"\\n===== COMPARING DATABASES FOR {sample.upper()} =====\")\n",
        "\n",
        "            # Get data for this sample\n",
        "            sample_data = df[df['actual_sample'] == sample]\n",
        "\n",
        "            # Get unique databases for this sample\n",
        "            dbs = sorted(sample_data['db_filename'].unique())\n",
        "\n",
        "            # Get spectra identified under each database\n",
        "            db_spectra = {}\n",
        "            for db in dbs:\n",
        "                db_df = sample_data[sample_data['db_filename'] == db]\n",
        "                spectra = set(db_df['specId'].astype(str))\n",
        "                db_spectra[db] = spectra\n",
        "                print(f\"  {db}: {len(spectra)} spectra identified\")\n",
        "\n",
        "            # Compare first two databases (for simplicity)\n",
        "            if len(dbs) >= 2:\n",
        "                db1, db2 = dbs[0], dbs[1]\n",
        "\n",
        "                # Calculate overlap\n",
        "                overlap = db_spectra[db1].intersection(db_spectra[db2])\n",
        "                only_db1 = db_spectra[db1] - db_spectra[db2]\n",
        "                only_db2 = db_spectra[db2] - db_spectra[db1]\n",
        "\n",
        "                # Calculate overlap percentages\n",
        "                overlap_pct_of_db1 = len(overlap) / len(db_spectra[db1]) * 100 if db_spectra[db1] else 0\n",
        "                overlap_pct_of_db2 = len(overlap) / len(db_spectra[db2]) * 100 if db_spectra[db2] else 0\n",
        "\n",
        "                print(f\"\\n  {db1} vs {db2}:\")\n",
        "                print(f\"    Overlap: {len(overlap)} spectra ({overlap_pct_of_db1:.1f}% of {db1}, {overlap_pct_of_db2:.1f}% of {db2})\")\n",
        "                print(f\"    Only in {db1}: {len(only_db1)} spectra\")\n",
        "                print(f\"    Only in {db2}: {len(only_db2)} spectra\")\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df.to_csv('/content/enriched_peps_parameters.csv', index=False)\n",
        "print(\"\\nEnriched DataFrame saved to: /content/enriched_peps_parameters.csv\")\n"
      ],
      "metadata": {
        "id": "htbhElkoztUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "This section provides a comprehensive overview of the dataframe structure and contents. The code:\n",
        "1. Shows the overall shape of the dataframe (number of rows and columns)\n",
        "2. Lists all column names with their indices for easy reference\n",
        "3. Displays the data types of each column\n",
        "4. Shows the first 5 rows of the dataframe\n",
        "\n",
        "This helps you understand what data you're working with and how it's structured before you begin your analysis."
      ],
      "metadata": {
        "id": "_KuTe2SMz3Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about the dataframe\n",
        "print(\"DataFrame shape (rows, columns):\", df.shape)\n",
        "\n",
        "# Display column names\n",
        "print(\"\\nColumn names:\")\n",
        "for i, col in enumerate(df.columns):\n",
        "    print(f\"{i+1}. {col}\")\n",
        "\n",
        "# Display data types of each column\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# First 5 rows\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "tmicDsIJ0AnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps for Analysis\n",
        "Now that the data is prepared and explored, you can add additional analysis cells to:\n",
        "\n",
        "1. **Filter peptides by sample type**:"
      ],
      "metadata": {
        "id": "0YvQ05s80Ib3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "   # Example: Select only peptides from the mammoth sample\n",
        "   mammoth_peptides = df[df['actual_sample'] == 'mammoth']\n",
        "   print(f\"Found {len(mammoth_peptides)} mammoth peptides\")"
      ],
      "metadata": {
        "id": "9d1X_wlw0Ogo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TfH3nYxQ0Te5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Analyze peptide modifications**:"
      ],
      "metadata": {
        "id": "bTyepnak0Qqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Count peptides with hydroxyproline modifications\n",
        "hyp_cols = [col for col in df.columns if 'hydroxy' in col.lower() and col.startswith('variable')]\n",
        "has_hyp = df[hyp_cols].any(axis=1)\n",
        "print(f\"Found {has_hyp.sum()} peptides with hydroxyproline modifications\")"
      ],
      "metadata": {
        "id": "2M-x4CYD0Xsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Compare identification efficiency across samples**:"
      ],
      "metadata": {
        "id": "VhWQeD940iuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compare how many unique peptides were identified per sample\n",
        "peptides_by_sample = df.groupby('actual_sample')['peptide'].nunique()\n",
        "print(\"Unique peptides by sample:\")\n",
        "print(peptides_by_sample)"
      ],
      "metadata": {
        "id": "j1w1_s500nen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Visualize score distributions**"
      ],
      "metadata": {
        "id": "KW8UymGF0r4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Create a histogram of peptide scores by sample\n",
        "plt.figure(figsize=(10, 6))\n",
        "for sample in df['actual_sample'].unique():\n",
        "  sample_data = df[df['actual_sample'] == sample]\n",
        "  plt.hist(sample_data['score'], alpha=0.5, label=sample, bins=20)\n",
        "plt.legend()\n",
        "plt.title('Distribution of Peptide Scores by Sample')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l58kSyYazbGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}