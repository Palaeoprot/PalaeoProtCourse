{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SYzs_ZvCXBC2cbSYUEdEzxR5hqKgSzyF",
      "authorship_tag": "ABX9TyPFeAONVwzsIrJsqMB3Vbyn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palaeoprot/PalaeoProtCourse/blob/main/Tartu_Excercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paleoproteomics Data Analysis Workflow\n",
        "\n",
        "This notebook is designed for the Paleoproteomics Course (March 17-21, 2025) to help you understand how to process and analyze mass spectrometry data from NovorCloud. Each section is explained to help you understand the purpose of the code and how it works.\n"
      ],
      "metadata": {
        "id": "ad2arFdPyeRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive\n",
        "This section mounts your Google Drive to access files stored there. This is necessary because we'll be working with data files from NovorCloud that are stored in Google Drive.\n"
      ],
      "metadata": {
        "id": "tSRyF97cdlYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (if using Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk3aakcocQP0",
        "outputId": "bb49da83-9bf4-4c80-f72b-0268fbcd1eac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the Environment\n",
        "This section imports all the required libraries for our analysis. We'll use several Python libraries:\n",
        "- Standard libraries for file handling, regular expressions, and data structures\n",
        "- NumPy and Pandas for data manipulation\n",
        "- Matplotlib and Seaborn for data visualization"
      ],
      "metadata": {
        "id": "ccWCQh38dV4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "import glob\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "bVypfOd8dZJS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Extraction and Organization\n",
        "This section identifies and extracts data from zip files that were downloaded from NovorCloud. The code:\n",
        "1. Looks for all zip files in the specified directory\n",
        "2. Creates an output directory for extracted files if it doesn't exist\n",
        "3. Extracts two types of files: parameters.js (search settings) and .peps.txt (peptide identifications)\n",
        "4. Parses the filenames to extract metadata (student, sample, conditions)\n",
        "5. Creates dataframes to track the extracted files and their metadata\n",
        "6. Saves this metadata to CSV files for reference"
      ],
      "metadata": {
        "id": "mBFIDBaZdh3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Define paths\n",
        "extracted_dir = '/content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files'\n",
        "output_csv = '/content/combined_peptides_parameters.csv'\n",
        "\n",
        "# Get lists of extracted files\n",
        "all_peps_files = glob.glob(os.path.join(extracted_dir, '*.peps.txt'))\n",
        "all_params_files = glob.glob(os.path.join(extracted_dir, '*_parameters.js'))  # Changed to .js extension\n",
        "\n",
        "print(f\"Found {len(all_peps_files)} peptide files and {len(all_params_files)} parameter files\")\n",
        "\n",
        "# Build dictionaries to map between peps files and parameter files\n",
        "param_mapping = {}\n",
        "\n",
        "# Create a mapping between peptide files and parameter files based on filename patterns\n",
        "for peps_file in all_peps_files:\n",
        "    peps_basename = os.path.basename(peps_file)\n",
        "\n",
        "    # Extract the prefix part (e.g., \"IGK-turkey-bostaurusfasta\")\n",
        "    prefix = peps_basename.split('_F_1')[0] if '_F_1' in peps_basename else peps_basename.split('.peps.txt')[0]\n",
        "\n",
        "    # Look for a matching parameter file\n",
        "    matching_param_file = None\n",
        "    param_filename = f\"{prefix}_parameters.js\"\n",
        "\n",
        "    for param_file in all_params_files:\n",
        "        if os.path.basename(param_file) == param_filename:\n",
        "            matching_param_file = param_file\n",
        "            break\n",
        "\n",
        "    if matching_param_file:\n",
        "        param_mapping[peps_file] = matching_param_file\n",
        "        print(f\"Matched: {os.path.basename(peps_file)} -> {os.path.basename(matching_param_file)}\")\n",
        "    else:\n",
        "        print(f\"No match found for: {os.path.basename(peps_file)}\")\n",
        "\n",
        "# Prepare list to hold DataFrames\n",
        "all_dfs = []\n",
        "\n",
        "# Process each peps.txt file\n",
        "for peps_path in all_peps_files:\n",
        "    peps_filename = os.path.basename(peps_path)\n",
        "    print(f\"\\nProcessing: {peps_filename}\")\n",
        "\n",
        "    # Get matching parameter file\n",
        "    param_path = param_mapping.get(peps_path)\n",
        "\n",
        "    if not param_path:\n",
        "        print(f\"  WARNING: No parameter file found for {peps_filename}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Read parameter file\n",
        "        with open(param_path, 'r') as f:\n",
        "            param_content = f.read()\n",
        "\n",
        "        # Extract JSON portion - specific pattern for your files\n",
        "        match = re.search(r'const parameters = (.+)', param_content)\n",
        "        if match:\n",
        "            param_json_str = match.group(1)\n",
        "            parameters = json.loads(param_json_str)\n",
        "            print(f\"  Successfully parsed parameters from {os.path.basename(param_path)}\")\n",
        "        else:\n",
        "            print(f\"  ERROR: Could not extract parameters from {param_path}\")\n",
        "            continue\n",
        "\n",
        "        # Read peptide file\n",
        "        try:\n",
        "            peps_df = pd.read_csv(\n",
        "                peps_path,\n",
        "                comment='#',\n",
        "                skip_blank_lines=True,\n",
        "                skipinitialspace=True,\n",
        "                on_bad_lines='warn'\n",
        "            )\n",
        "\n",
        "            if peps_df.empty:\n",
        "                print(f\"  WARNING: Empty peptide file: {peps_filename}\")\n",
        "                continue\n",
        "\n",
        "            # Extract metadata from filename\n",
        "            filename_parts = peps_filename.split('_F_1')[0]\n",
        "\n",
        "            # Try to extract student/sample/conditions\n",
        "            if '-' in filename_parts:\n",
        "                parts = filename_parts.split('-')\n",
        "                peps_df['student'] = parts[0]\n",
        "                if len(parts) > 1:\n",
        "                    peps_df['sample'] = parts[1]\n",
        "                if len(parts) > 2:\n",
        "                    peps_df['conditions'] = parts[2]\n",
        "            else:\n",
        "                parts = filename_parts.split('_')\n",
        "                peps_df['student'] = parts[0]\n",
        "                if len(parts) > 1:\n",
        "                    peps_df['sample'] = '_'.join(parts[1:])\n",
        "\n",
        "            # Add parameter data\n",
        "            for key, value in parameters.items():\n",
        "                if isinstance(value, (list, dict)):\n",
        "                    peps_df[key] = json.dumps(value)\n",
        "                else:\n",
        "                    peps_df[key] = value\n",
        "\n",
        "            # Add reference columns\n",
        "            peps_df['peps_file'] = peps_filename\n",
        "            peps_df['param_file'] = os.path.basename(param_path)\n",
        "\n",
        "            # Add to list of dataframes\n",
        "            all_dfs.append(peps_df)\n",
        "            print(f\"  Added {len(peps_df)} rows from {peps_filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ERROR reading peptide file {peps_path}: {str(e)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR processing parameter file {param_path}: {str(e)}\")\n",
        "\n",
        "# Combine all dataframes\n",
        "if all_dfs:\n",
        "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "    combined_df.to_csv(output_csv, index=False)\n",
        "    print(f\"\\n✅ Combined DataFrame saved to: {output_csv}\")\n",
        "    print(f\"Total rows: {len(combined_df)}\")\n",
        "    print(f\"Total unique peptides: {combined_df['peptide'].nunique()}\")\n",
        "\n",
        "    # Basic statistics\n",
        "    if 'score' in combined_df.columns:\n",
        "        print(f\"Score statistics: min={combined_df['score'].min()}, max={combined_df['score'].max()}, mean={combined_df['score'].mean():.2f}\")\n",
        "else:\n",
        "    print(\"No peptide files processed.\")"
      ],
      "metadata": {
        "id": "h8T3C6meA0Ox",
        "outputId": "2d211777-6eac-4a43-e6c6-a33ee2cb0739",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7 peptide files and 7 parameter files\n",
            "Matched: IGK-turkey-bostaurusfasta_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> IGK-turkey-bostaurusfasta_parameters.js\n",
            "Matched: Pauline_Mammoth_Swissprot_F_1 - 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt -> Pauline_Mammoth_Swissprot_parameters.js\n",
            "Matched: MC-cow-cowfasta_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> MC-cow-cowfasta_parameters.js\n",
            "Matched: RB_swissprot_mammoth_F_1 - 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt -> RB_swissprot_mammoth_parameters.js\n",
            "Matched: KT_cow_cowfasta_(AutoEnzymes)_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> KT_cow_cowfasta_(AutoEnzymes)_parameters.js\n",
            "Matched: SS_cow_cowfasta_AutoEnzymes_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt -> SS_cow_cowfasta_AutoEnzymes_parameters.js\n",
            "Matched: LS_Edmontosaur_bos_taurus_F_1 - 290423_Edmontosaur_1hr_Neat_290423_Edmontosaur_1hr_Neat.peps.txt -> LS_Edmontosaur_bos_taurus_parameters.js\n",
            "\n",
            "Processing: IGK-turkey-bostaurusfasta_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  Successfully parsed parameters from IGK-turkey-bostaurusfasta_parameters.js\n",
            "  Added 6110 rows from IGK-turkey-bostaurusfasta_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: Pauline_Mammoth_Swissprot_F_1 - 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "  Successfully parsed parameters from Pauline_Mammoth_Swissprot_parameters.js\n",
            "  Added 25309 rows from Pauline_Mammoth_Swissprot_F_1 - 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "\n",
            "Processing: MC-cow-cowfasta_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  Successfully parsed parameters from MC-cow-cowfasta_parameters.js\n",
            "  Added 6142 rows from MC-cow-cowfasta_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: RB_swissprot_mammoth_F_1 - 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "  Successfully parsed parameters from RB_swissprot_mammoth_parameters.js\n",
            "  Added 25148 rows from RB_swissprot_mammoth_F_1 - 20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.peps.txt\n",
            "\n",
            "Processing: KT_cow_cowfasta_(AutoEnzymes)_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  Successfully parsed parameters from KT_cow_cowfasta_(AutoEnzymes)_parameters.js\n",
            "  Added 6117 rows from KT_cow_cowfasta_(AutoEnzymes)_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: SS_cow_cowfasta_AutoEnzymes_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "  Successfully parsed parameters from SS_cow_cowfasta_AutoEnzymes_parameters.js\n",
            "  Added 6131 rows from SS_cow_cowfasta_AutoEnzymes_F_1 - 260423_Bovine_Collagen_1hr_1_in_1000_260423_Bovine_Collagen_1hr_1_in_1000.peps.txt\n",
            "\n",
            "Processing: LS_Edmontosaur_bos_taurus_F_1 - 290423_Edmontosaur_1hr_Neat_290423_Edmontosaur_1hr_Neat.peps.txt\n",
            "  Successfully parsed parameters from LS_Edmontosaur_bos_taurus_parameters.js\n",
            "  Added 5962 rows from LS_Edmontosaur_bos_taurus_F_1 - 290423_Edmontosaur_1hr_Neat_290423_Edmontosaur_1hr_Neat.peps.txt\n",
            "\n",
            "✅ Combined DataFrame saved to: /content/combined_peptides_parameters.csv\n",
            "Total rows: 80919\n",
            "Total unique peptides: 59130\n",
            "Score statistics: min=-2.6647, max=33.4171, mean=1.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enriching the Data\n",
        "This section loads the combined data file and enhances it by processing the various parameters from the search. The code:\n",
        "1. Loads the previously combined peptide and parameter data\n",
        "2. Processes parameter columns that contain complex data (like lists of PTMs)\n",
        "3. Creates boolean columns for each type of parameter (e.g., each enzyme type, each fixed or variable PTM)\n",
        "4. Extracts raw filename and database filename information\n",
        "5. Parses actual sample types based on raw filenames\n",
        "6. Provides detailed information about the updated dataframe\n",
        "7. Saves the enriched dataframe to a new CSV file\n",
        "\n",
        "\n",
        "This enrichment makes the data easier to analyze because it transforms complex nested data into simple boolean columns.\n",
        "\n"
      ],
      "metadata": {
        "id": "vEFurbyMy_Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the combined data\n",
        "file_path = '/content/combined_peptides_parameters.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(\"Initial DataFrame shape (rows, columns):\", df.shape)\n",
        "\n",
        "# Process parameter columns that are currently stored as strings but represent lists or objects\n",
        "params_to_process = [\n",
        "    'spectraFilenames',\n",
        "    'dbFilenames',\n",
        "    'fixedPtms',\n",
        "    'variablePtms'\n",
        "]\n",
        "\n",
        "# Extract values from string representations of lists\n",
        "for param in params_to_process:\n",
        "    if param in df.columns:\n",
        "        # Create new columns for each item in these lists\n",
        "        try:\n",
        "            # First, try to parse the string representations of lists\n",
        "            new_values = {}\n",
        "\n",
        "            for idx, value in df[param].items():\n",
        "                if pd.isna(value):\n",
        "                    continue\n",
        "\n",
        "                # Handle string representations of lists\n",
        "                if isinstance(value, str):\n",
        "                    # Clean up the string to make it properly parseable\n",
        "                    clean_value = value.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
        "\n",
        "                    try:\n",
        "                        # Try to parse as JSON\n",
        "                        parsed = json.loads(clean_value)\n",
        "                    except:\n",
        "                        try:\n",
        "                            # Try to parse as Python literal\n",
        "                            parsed = ast.literal_eval(value)\n",
        "                        except:\n",
        "                            # If all parsing fails, just use the string as is\n",
        "                            parsed = value\n",
        "\n",
        "                    new_values[idx] = parsed\n",
        "                else:\n",
        "                    new_values[idx] = value\n",
        "\n",
        "            # Create a new series with the parsed values\n",
        "            parsed_series = pd.Series(new_values)\n",
        "\n",
        "            # For list columns, create individual columns for each item\n",
        "            if len(parsed_series) > 0 and isinstance(parsed_series.iloc[0], list):\n",
        "                # Get all unique values across all lists\n",
        "                all_items = set()\n",
        "                for idx, items in parsed_series.items():\n",
        "                    if isinstance(items, list):\n",
        "                        all_items.update(items)\n",
        "\n",
        "                # Create a column for each item\n",
        "                for item in sorted(all_items):\n",
        "                    col_name = f\"{param}_{item.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "                    df[col_name] = False\n",
        "\n",
        "                    # Mark True where the item exists in the list\n",
        "                    for idx, items in parsed_series.items():\n",
        "                        if isinstance(items, list) and item in items:\n",
        "                            df.at[idx, col_name] = True\n",
        "\n",
        "            # For scalar values, just keep the original column\n",
        "            else:\n",
        "                df[param] = parsed_series\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {param}: {e}\")\n",
        "\n",
        "# Extract raw filename from spectraFilenames\n",
        "if 'spectraFilenames' in df.columns:\n",
        "    def get_first_raw_file(value):\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if isinstance(value, list) and len(value) > 0:\n",
        "                return value[0]\n",
        "            elif isinstance(value, str):\n",
        "                # Try to parse as a list if it looks like one\n",
        "                if value.startswith('[') and value.endswith(']'):\n",
        "                    parsed = ast.literal_eval(value)\n",
        "                    if isinstance(parsed, list) and len(parsed) > 0:\n",
        "                        return parsed[0]\n",
        "                return value\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    df['raw_filename'] = df['spectraFilenames'].apply(get_first_raw_file)\n",
        "\n",
        "# Extract database filename from dbFilenames\n",
        "if 'dbFilenames' in df.columns:\n",
        "    def get_first_db_file(value):\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if isinstance(value, list) and len(value) > 0:\n",
        "                return value[0]\n",
        "            elif isinstance(value, str):\n",
        "                # Try to parse as a list if it looks like one\n",
        "                if value.startswith('[') and value.endswith(']'):\n",
        "                    parsed = ast.literal_eval(value)\n",
        "                    if isinstance(parsed, list) and len(parsed) > 0:\n",
        "                        return parsed[0]\n",
        "                return value\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    df['db_filename'] = df['dbFilenames'].apply(get_first_db_file)\n",
        "\n",
        "# Extract enzyme information\n",
        "if 'enzymeSelection' in df.columns:\n",
        "    # Create boolean columns for each enzyme type\n",
        "    unique_enzymes = df['enzymeSelection'].unique()\n",
        "    for enzyme in unique_enzymes:\n",
        "        if pd.notna(enzyme):\n",
        "            col_name = f\"enzyme_{enzyme}\"\n",
        "            df[col_name] = df['enzymeSelection'] == enzyme\n",
        "\n",
        "# Extract PTM information into separate columns\n",
        "def create_ptm_columns(row):\n",
        "    \"\"\"Create boolean columns for each PTM type\"\"\"\n",
        "    result = {}\n",
        "\n",
        "    # Process fixed PTMs\n",
        "    if 'fixedPtms' in df.columns:\n",
        "        fixed_ptms = row['fixedPtms']\n",
        "        if isinstance(fixed_ptms, list):\n",
        "            for ptm in fixed_ptms:\n",
        "                result[f\"fixed_ptm_{ptm.replace(' ', '_')}\"] = True\n",
        "\n",
        "    # Process variable PTMs\n",
        "    if 'variablePtms' in df.columns:\n",
        "        var_ptms = row['variablePtms']\n",
        "        if isinstance(var_ptms, list):\n",
        "            for ptm in var_ptms:\n",
        "                result[f\"var_ptm_{ptm.replace(' ', '_')}\"] = True\n",
        "\n",
        "    return result\n",
        "\n",
        "# Apply the function and add the columns\n",
        "if 'fixedPtms' in df.columns or 'variablePtms' in df.columns:\n",
        "    # First create all possible PTM columns\n",
        "    all_fixed_ptms = set()\n",
        "    all_var_ptms = set()\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Fixed PTMs\n",
        "        if 'fixedPtms' in df.columns:\n",
        "            fixed_ptms = row['fixedPtms']\n",
        "            if isinstance(fixed_ptms, list):\n",
        "                all_fixed_ptms.update(fixed_ptms)\n",
        "\n",
        "        # Variable PTMs\n",
        "        if 'variablePtms' in df.columns:\n",
        "            var_ptms = row['variablePtms']\n",
        "            if isinstance(var_ptms, list):\n",
        "                all_var_ptms.update(var_ptms)\n",
        "\n",
        "    # Create columns for all PTMs\n",
        "    for ptm in all_fixed_ptms:\n",
        "        col_name = f\"fixed_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "        df[col_name] = False\n",
        "\n",
        "    for ptm in all_var_ptms:\n",
        "        col_name = f\"var_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "        df[col_name] = False\n",
        "\n",
        "    # Now fill in the values\n",
        "    for idx, row in df.iterrows():\n",
        "        # Fixed PTMs\n",
        "        if 'fixedPtms' in df.columns:\n",
        "            fixed_ptms = row['fixedPtms']\n",
        "            if isinstance(fixed_ptms, list):\n",
        "                for ptm in fixed_ptms:\n",
        "                    col_name = f\"fixed_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "                    df.at[idx, col_name] = True\n",
        "\n",
        "        # Variable PTMs\n",
        "        if 'variablePtms' in df.columns:\n",
        "            var_ptms = row['variablePtms']\n",
        "            if isinstance(var_ptms, list):\n",
        "                for ptm in var_ptms:\n",
        "                    col_name = f\"var_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "                    df.at[idx, col_name] = True\n",
        "\n",
        "# Extract sample from raw filename\n",
        "def extract_sample_from_raw(raw_filename):\n",
        "    if pd.isna(raw_filename):\n",
        "        return \"unknown\"\n",
        "\n",
        "    raw_str = str(raw_filename).lower()\n",
        "\n",
        "    if 'bovine' in raw_str or 'cow' in raw_str:\n",
        "        return \"cow\"\n",
        "    elif 'mammoth' in raw_str:\n",
        "        return \"mammoth\"\n",
        "    elif 'edmontosaur' in raw_str:\n",
        "        return \"edmontosaurus\"\n",
        "    elif 'turkey' in raw_str:\n",
        "        return \"turkey\"\n",
        "    else:\n",
        "        return raw_str.split('.')[0]\n",
        "\n",
        "# Add actual_sample column based on raw filename\n",
        "if 'raw_filename' in df.columns:\n",
        "    df['actual_sample'] = df['raw_filename'].apply(extract_sample_from_raw)\n",
        "\n",
        "# Print information about the updated dataframe\n",
        "print(\"\\nUpdated DataFrame shape (rows, columns):\", df.shape)\n",
        "print(\"\\nNew columns added:\")\n",
        "new_cols = set(df.columns) - set(['line', 'target/decoy', 'nDecoy', 'specId', 'scanNum', 'mz', 'z', 'ppm', 'score',\n",
        "                             'peptide', 'protein', 'student', 'sample', 'conditions', 'original_filename', 'notes',\n",
        "                             'jobTitle', 'spectraFilenames', 'dbFilenames', 'fixedPtms', 'variablePtms',\n",
        "                             'enzymeSelection', 'fragmentationSelection', 'massAnalyzer', 'precursorErrorTol',\n",
        "                             'precursorErrorTolUnit', 'fragmentErrorTol', 'fragmentErrorTolUnit',\n",
        "                             'activateBuiltInPtms', 'peps_file', 'param_file', 'condition'])\n",
        "for col in sorted(new_cols):\n",
        "    print(f\"  {col}\")\n",
        "\n",
        "# Show sample distribution\n",
        "if 'actual_sample' in df.columns:\n",
        "    print(\"\\nSample distribution:\")\n",
        "    sample_counts = df['actual_sample'].value_counts()\n",
        "    for sample, count in sample_counts.items():\n",
        "        print(f\"  {sample}: {count} entries\")\n",
        "\n",
        "# Find unique combinations of samples and methods\n",
        "if 'actual_sample' in df.columns:\n",
        "    # Use database file as a proxy for search strategy\n",
        "    if 'db_filename' in df.columns:\n",
        "        print(\"\\nUnique sample-database combinations:\")\n",
        "        combinations = df.groupby(['actual_sample', 'db_filename']).size().reset_index(name='count')\n",
        "        for _, row in combinations.iterrows():\n",
        "            print(f\"  {row['actual_sample']} with {row['db_filename']}: {row['count']} entries\")\n",
        "\n",
        "        # Find samples analyzed with multiple databases\n",
        "        sample_db_counts = df.groupby('actual_sample')['db_filename'].nunique()\n",
        "        samples_with_multiple_dbs = sample_db_counts[sample_db_counts > 1].index.tolist()\n",
        "\n",
        "        print(f\"\\nSamples analyzed with multiple databases: {samples_with_multiple_dbs}\")\n",
        "\n",
        "        # For each such sample, show the databases\n",
        "        for sample in samples_with_multiple_dbs:\n",
        "            dbs = sorted(df[df['actual_sample'] == sample]['db_filename'].unique())\n",
        "            print(f\"  {sample}:\")\n",
        "            for db in dbs:\n",
        "                count = len(df[(df['actual_sample'] == sample) & (df['db_filename'] == db)])\n",
        "                print(f\"    {db}: {count} entries\")\n",
        "\n",
        "        # Now, for each sample with multiple databases, compare the spectrum identifications\n",
        "        for sample in samples_with_multiple_dbs:\n",
        "            print(f\"\\n===== COMPARING DATABASES FOR {sample.upper()} =====\")\n",
        "\n",
        "            # Get data for this sample\n",
        "            sample_data = df[df['actual_sample'] == sample]\n",
        "\n",
        "            # Get unique databases for this sample\n",
        "            dbs = sorted(sample_data['db_filename'].unique())\n",
        "\n",
        "            # Get spectra identified under each database\n",
        "            db_spectra = {}\n",
        "            for db in dbs:\n",
        "                db_df = sample_data[sample_data['db_filename'] == db]\n",
        "                spectra = set(db_df['specId'].astype(str))\n",
        "                db_spectra[db] = spectra\n",
        "                print(f\"  {db}: {len(spectra)} spectra identified\")\n",
        "\n",
        "            # Compare first two databases (for simplicity)\n",
        "            if len(dbs) >= 2:\n",
        "                db1, db2 = dbs[0], dbs[1]\n",
        "\n",
        "                # Calculate overlap\n",
        "                overlap = db_spectra[db1].intersection(db_spectra[db2])\n",
        "                only_db1 = db_spectra[db1] - db_spectra[db2]\n",
        "                only_db2 = db_spectra[db2] - db_spectra[db1]\n",
        "\n",
        "                # Calculate overlap percentages\n",
        "                overlap_pct_of_db1 = len(overlap) / len(db_spectra[db1]) * 100 if db_spectra[db1] else 0\n",
        "                overlap_pct_of_db2 = len(overlap) / len(db_spectra[db2]) * 100 if db_spectra[db2] else 0\n",
        "\n",
        "                print(f\"\\n  {db1} vs {db2}:\")\n",
        "                print(f\"    Overlap: {len(overlap)} spectra ({overlap_pct_of_db1:.1f}% of {db1}, {overlap_pct_of_db2:.1f}% of {db2})\")\n",
        "                print(f\"    Only in {db1}: {len(only_db1)} spectra\")\n",
        "                print(f\"    Only in {db2}: {len(only_db2)} spectra\")\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df.to_csv('/content/enriched_peps_parameters.csv', index=False)\n",
        "print(\"\\nEnriched DataFrame saved to: /content/enriched_peps_parameters.csv\")\n"
      ],
      "metadata": {
        "id": "htbhElkoztUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec4b72c-dae1-4768-f9db-3249af495ddc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-ea766c1637d9>:3: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial DataFrame shape (rows, columns): (80919, 30)\n",
            "\n",
            "Updated DataFrame shape (rows, columns): (80919, 52)\n",
            "\n",
            "New columns added:\n",
            "  actual_sample\n",
            "  dbFilenames_bos_taurus.fasta\n",
            "  dbFilenames_uniprot_sprot.fasta\n",
            "  db_filename\n",
            "  enzyme_Auto\n",
            "  enzyme_Trypsin\n",
            "  fixedPtms_Carbamidomethyl_C\n",
            "  raw_filename\n",
            "  spectraFilenames_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.raw\n",
            "  spectraFilenames_260423_Bovine_Collagen_1hr_1_in_1000.raw\n",
            "  spectraFilenames_290423_Edmontosaur_1hr_Neat.raw\n",
            "  variablePtms_Carbamidomethyl_C\n",
            "  variablePtms_Deamidated_N\n",
            "  variablePtms_Deamidated_NQ\n",
            "  variablePtms_Hydroxproline\n",
            "  variablePtms_HydroxyProline\n",
            "  variablePtms_Hydroxyproline\n",
            "  variablePtms_Oxidation_M\n",
            "  variablePtms_ProlineOxydation\n",
            "  variablePtms_Pyro-Glu_E\n",
            "  variablePtms_Pyro-Glu_Q\n",
            "  variablePtms_hydroxyproline\n",
            "\n",
            "Sample distribution:\n",
            "  unknown: 80919 entries\n",
            "\n",
            "Unique sample-database combinations:\n",
            "\n",
            "Samples analyzed with multiple databases: []\n",
            "\n",
            "Enriched DataFrame saved to: /content/enriched_peps_parameters.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "This section provides a comprehensive overview of the dataframe structure and contents. The code:\n",
        "1. Shows the overall shape of the dataframe (number of rows and columns)\n",
        "2. Lists all column names with their indices for easy reference\n",
        "3. Displays the data types of each column\n",
        "4. Shows the first 5 rows of the dataframe\n",
        "\n",
        "This helps you understand what data you're working with and how it's structured before you begin your analysis."
      ],
      "metadata": {
        "id": "_KuTe2SMz3Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about the dataframe\n",
        "print(\"DataFrame shape (rows, columns):\", df.shape)\n",
        "\n",
        "# Display column names\n",
        "print(\"\\nColumn names:\")\n",
        "for i, col in enumerate(df.columns):\n",
        "    print(f\"{i+1}. {col}\")\n",
        "\n",
        "# Display data types of each column\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# First 5 rows\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "tmicDsIJ0AnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3fa24b-ca30-4179-9e74-927ea492361c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame shape (rows, columns): (80919, 52)\n",
            "\n",
            "Column names:\n",
            "1. line\n",
            "2. target/decoy\n",
            "3. nDecoy\n",
            "4. specId\n",
            "5. scanNum\n",
            "6. mz\n",
            "7. z\n",
            "8. ppm\n",
            "9. score\n",
            "10. peptide\n",
            "11. protein\n",
            "12. student\n",
            "13. sample\n",
            "14. conditions\n",
            "15. notes\n",
            "16. jobTitle\n",
            "17. spectraFilenames\n",
            "18. dbFilenames\n",
            "19. fixedPtms\n",
            "20. variablePtms\n",
            "21. enzymeSelection\n",
            "22. fragmentationSelection\n",
            "23. massAnalyzer\n",
            "24. precursorErrorTol\n",
            "25. precursorErrorTolUnit\n",
            "26. fragmentErrorTol\n",
            "27. fragmentErrorTolUnit\n",
            "28. activateBuiltInPtms\n",
            "29. peps_file\n",
            "30. param_file\n",
            "31. spectraFilenames_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.raw\n",
            "32. spectraFilenames_260423_Bovine_Collagen_1hr_1_in_1000.raw\n",
            "33. spectraFilenames_290423_Edmontosaur_1hr_Neat.raw\n",
            "34. dbFilenames_bos_taurus.fasta\n",
            "35. dbFilenames_uniprot_sprot.fasta\n",
            "36. fixedPtms_Carbamidomethyl_C\n",
            "37. variablePtms_Carbamidomethyl_C\n",
            "38. variablePtms_Deamidated_N\n",
            "39. variablePtms_Deamidated_NQ\n",
            "40. variablePtms_Hydroxproline\n",
            "41. variablePtms_HydroxyProline\n",
            "42. variablePtms_Hydroxyproline\n",
            "43. variablePtms_Oxidation_M\n",
            "44. variablePtms_ProlineOxydation\n",
            "45. variablePtms_Pyro-Glu_E\n",
            "46. variablePtms_Pyro-Glu_Q\n",
            "47. variablePtms_hydroxyproline\n",
            "48. raw_filename\n",
            "49. db_filename\n",
            "50. enzyme_Trypsin\n",
            "51. enzyme_Auto\n",
            "52. actual_sample\n",
            "\n",
            "Data types:\n",
            "line                                                                             int64\n",
            "target/decoy                                                                    object\n",
            "nDecoy                                                                           int64\n",
            "specId                                                                           int64\n",
            "scanNum                                                                          int64\n",
            "mz                                                                             float64\n",
            "z                                                                                int64\n",
            "ppm                                                                            float64\n",
            "score                                                                          float64\n",
            "peptide                                                                         object\n",
            "protein                                                                         object\n",
            "student                                                                         object\n",
            "sample                                                                          object\n",
            "conditions                                                                      object\n",
            "notes                                                                          float64\n",
            "jobTitle                                                                        object\n",
            "spectraFilenames                                                                object\n",
            "dbFilenames                                                                     object\n",
            "fixedPtms                                                                       object\n",
            "variablePtms                                                                    object\n",
            "enzymeSelection                                                                 object\n",
            "fragmentationSelection                                                          object\n",
            "massAnalyzer                                                                    object\n",
            "precursorErrorTol                                                              float64\n",
            "precursorErrorTolUnit                                                           object\n",
            "fragmentErrorTol                                                               float64\n",
            "fragmentErrorTolUnit                                                            object\n",
            "activateBuiltInPtms                                                               bool\n",
            "peps_file                                                                       object\n",
            "param_file                                                                      object\n",
            "spectraFilenames_20190626_mammoth_QE7_nLC7_DS_SA_Tryptic_mammoth_sup_01.raw       bool\n",
            "spectraFilenames_260423_Bovine_Collagen_1hr_1_in_1000.raw                         bool\n",
            "spectraFilenames_290423_Edmontosaur_1hr_Neat.raw                                  bool\n",
            "dbFilenames_bos_taurus.fasta                                                      bool\n",
            "dbFilenames_uniprot_sprot.fasta                                                   bool\n",
            "fixedPtms_Carbamidomethyl_C                                                       bool\n",
            "variablePtms_Carbamidomethyl_C                                                    bool\n",
            "variablePtms_Deamidated_N                                                         bool\n",
            "variablePtms_Deamidated_NQ                                                        bool\n",
            "variablePtms_Hydroxproline                                                        bool\n",
            "variablePtms_HydroxyProline                                                       bool\n",
            "variablePtms_Hydroxyproline                                                       bool\n",
            "variablePtms_Oxidation_M                                                          bool\n",
            "variablePtms_ProlineOxydation                                                     bool\n",
            "variablePtms_Pyro-Glu_E                                                           bool\n",
            "variablePtms_Pyro-Glu_Q                                                           bool\n",
            "variablePtms_hydroxyproline                                                       bool\n",
            "raw_filename                                                                    object\n",
            "db_filename                                                                     object\n",
            "enzyme_Trypsin                                                                    bool\n",
            "enzyme_Auto                                                                       bool\n",
            "actual_sample                                                                   object\n",
            "dtype: object\n",
            "\n",
            "First 5 rows:\n",
            "   line target/decoy  nDecoy  specId  scanNum        mz  z  ppm    score  \\\n",
            "0     1       target       0    2804     5860  1345.972  3  2.0  33.4171   \n",
            "1     2       target       0    6591     6230  1009.986  4  0.4  29.7917   \n",
            "2     3       target       0    1816     3966  1196.566  3 -3.2  25.2497   \n",
            "3     4       target       0    2824     4006  1210.582  3 -2.8  23.6676   \n",
            "4     5       target       0    6018     5476  1427.203  2 -4.6  22.8971   \n",
            "\n",
            "                                             peptide  ...  \\\n",
            "0  GENGPVGPTGPVGAAGPSGPNGPP(Hyp)GPAGSRGDGGPP(Hyp)...  ...   \n",
            "1  GFP(Hyp)GLP(Hyp)GPSGEPGKQGPSGASGERGPP(Hyp)GPMG...  ...   \n",
            "2  GNSGEPGAP(Hyp)GSKGDTGAKGEPGPTGIQGPP(Hyp)GPAGEEGKR  ...   \n",
            "3  GLVGEP(Hyp)GPAGSKGESGNKGEPGAVGQP(Hyp)GPP(Hyp)G...  ...   \n",
            "4        GLTGPIGPP(Hyp)GPAGAP(Hyp)GDKGEAGPSGPAGPTGAR  ...   \n",
            "\n",
            "  variablePtms_Oxidation_M variablePtms_ProlineOxydation  \\\n",
            "0                     True                         False   \n",
            "1                     True                         False   \n",
            "2                     True                         False   \n",
            "3                     True                         False   \n",
            "4                     True                         False   \n",
            "\n",
            "  variablePtms_Pyro-Glu_E variablePtms_Pyro-Glu_Q  \\\n",
            "0                    True                    True   \n",
            "1                    True                    True   \n",
            "2                    True                    True   \n",
            "3                    True                    True   \n",
            "4                    True                    True   \n",
            "\n",
            "   variablePtms_hydroxyproline raw_filename db_filename enzyme_Trypsin  \\\n",
            "0                        False         None        None           True   \n",
            "1                        False         None        None           True   \n",
            "2                        False         None        None           True   \n",
            "3                        False         None        None           True   \n",
            "4                        False         None        None           True   \n",
            "\n",
            "  enzyme_Auto actual_sample  \n",
            "0       False       unknown  \n",
            "1       False       unknown  \n",
            "2       False       unknown  \n",
            "3       False       unknown  \n",
            "4       False       unknown  \n",
            "\n",
            "[5 rows x 52 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps for Analysis\n",
        "Now that the data is prepared and explored, you can add additional analysis cells to:\n",
        "\n",
        "1. **Filter peptides by sample type**:"
      ],
      "metadata": {
        "id": "0YvQ05s80Ib3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "   # Example: Select only peptides from the mammoth sample\n",
        "   mammoth_peptides = df[df['actual_sample'] == 'mammoth']\n",
        "   print(f\"Found {len(mammoth_peptides)} mammoth peptides\")"
      ],
      "metadata": {
        "id": "9d1X_wlw0Ogo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5787fea6-ee34-4ad3-ef5f-20996a23049c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 mammoth peptides\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TfH3nYxQ0Te5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Analyze peptide modifications**:"
      ],
      "metadata": {
        "id": "bTyepnak0Qqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Count peptides with hydroxyproline modifications\n",
        "hyp_cols = [col for col in df.columns if 'hydroxy' in col.lower() and col.startswith('variable')]\n",
        "has_hyp = df[hyp_cols].any(axis=1)\n",
        "print(f\"Found {has_hyp.sum()} peptides with hydroxyproline modifications\")"
      ],
      "metadata": {
        "id": "2M-x4CYD0Xsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6db588-1ee0-41f3-a8a2-92aa20402e12"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 49661 peptides with hydroxyproline modifications\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Compare identification efficiency across samples**:"
      ],
      "metadata": {
        "id": "VhWQeD940iuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compare how many unique peptides were identified per sample\n",
        "peptides_by_sample = df.groupby('actual_sample')['peptide'].nunique()\n",
        "print(\"Unique peptides by sample:\")\n",
        "print(peptides_by_sample)"
      ],
      "metadata": {
        "id": "j1w1_s500nen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Visualize score distributions**"
      ],
      "metadata": {
        "id": "KW8UymGF0r4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Create a histogram of peptide scores by sample\n",
        "plt.figure(figsize=(10, 6))\n",
        "for sample in df['actual_sample'].unique():\n",
        "  sample_data = df[df['actual_sample'] == sample]\n",
        "  plt.hist(sample_data['score'], alpha=0.5, label=sample, bins=20)\n",
        "plt.legend()\n",
        "plt.title('Distribution of Peptide Scores by Sample')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l58kSyYazbGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}