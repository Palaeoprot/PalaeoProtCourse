{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SYzs_ZvCXBC2cbSYUEdEzxR5hqKgSzyF",
      "authorship_tag": "ABX9TyMfvxc1fXptW4XkSjizuKs3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palaeoprot/PalaeoProtCourse/blob/main/Tartu_Excercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paleoproteomics Data Analysis Workflow\n",
        "\n",
        "This notebook is designed for the Paleoproteomics Course (March 17-21, 2025) to help you understand how to process and analyze mass spectrometry data from NovorCloud. Each section is explained to help you understand the purpose of the code and how it works.\n"
      ],
      "metadata": {
        "id": "ad2arFdPyeRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive\n",
        "This section mounts your Google Drive to access files stored there. This is necessary because we'll be working with data files from NovorCloud that are stored in Google Drive.\n"
      ],
      "metadata": {
        "id": "tSRyF97cdlYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (if using Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk3aakcocQP0",
        "outputId": "83b76c95-13a9-4926-879f-9b10f66879ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the Environment\n",
        "This section imports all the required libraries for our analysis. We'll use several Python libraries:\n",
        "- Standard libraries for file handling, regular expressions, and data structures\n",
        "- NumPy and Pandas for data manipulation\n",
        "- Matplotlib and Seaborn for data visualization"
      ],
      "metadata": {
        "id": "ccWCQh38dV4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "from glob import glob\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "bVypfOd8dZJS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Extraction and Organization\n",
        "This section identifies and extracts data from zip files that were downloaded from NovorCloud. The code:\n",
        "1. Looks for all zip files in the specified directory\n",
        "2. Creates an output directory for extracted files if it doesn't exist\n",
        "3. Extracts two types of files: parameters.js (search settings) and .peps.txt (peptide identifications)\n",
        "4. Parses the filenames to extract metadata (student, sample, conditions)\n",
        "5. Creates dataframes to track the extracted files and their metadata\n",
        "6. Saves this metadata to CSV files for reference"
      ],
      "metadata": {
        "id": "mBFIDBaZdh3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the paths\n",
        "base_path = '/content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/NovorCloud Output'\n",
        "extracted_dir = '/content/drive/MyDrive/2_Teaching/4 External Courses/Tartu Proteomics Course/Wednesday/Extracted_Files'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(extracted_dir, exist_ok=True)\n",
        "\n",
        "# Process all zip files in the directory\n",
        "zip_files = glob.glob(os.path.join(base_path, '*.zip'))\n",
        "print(f\"Found {len(zip_files)} zip files\")\n",
        "\n",
        "# Track extracted files\n",
        "extracted_peps_files = []\n",
        "extracted_params_files = []\n",
        "\n",
        "# Process each zip file\n",
        "for zip_path in zip_files:\n",
        "    zip_name = os.path.basename(zip_path).split('.')[0]  # Get filename without extension\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # List all files in the zip\n",
        "        file_list = zip_ref.namelist()\n",
        "\n",
        "        # Extract parameters.js\n",
        "        params_files = [f for f in file_list if f.endswith('parameters.js')]\n",
        "        for params_file in params_files:\n",
        "            # Create output path with zip name prefix\n",
        "            output_file = os.path.join(extracted_dir, f\"{zip_name}_parameters.js\")\n",
        "\n",
        "            # Extract the file\n",
        "            with zip_ref.open(params_file) as source, open(output_file, 'wb') as target:\n",
        "                target.write(source.read())\n",
        "\n",
        "            extracted_params_files.append(output_file)\n",
        "            print(f\"Extracted: {params_file} to {output_file}\")\n",
        "\n",
        "        # Extract .peps.txt files from subfolders\n",
        "        peps_files = [f for f in file_list if f.endswith('.peps.txt')]\n",
        "        for peps_file in peps_files:\n",
        "            # Get subfolder name from the path\n",
        "            subfolder = os.path.dirname(peps_file).split('/')[-1]\n",
        "            # Create descriptive output filename\n",
        "            peps_base = os.path.basename(peps_file)\n",
        "            output_file = os.path.join(extracted_dir, f\"{zip_name}_{subfolder}_{peps_base}\")\n",
        "\n",
        "            # Extract the file\n",
        "            with zip_ref.open(peps_file) as source, open(output_file, 'wb') as target:\n",
        "                target.write(source.read())\n",
        "\n",
        "            extracted_peps_files.append(output_file)\n",
        "            print(f\"Extracted: {peps_file} to {output_file}\")\n",
        "\n",
        "print(f\"\\nExtraction complete:\")\n",
        "print(f\"- {len(extracted_params_files)} parameters.js files extracted\")\n",
        "print(f\"- {len(extracted_peps_files)} .peps.txt files extracted\")\n",
        "\n",
        "# Parse filenames to extract metadata\n",
        "def parse_extracted_filename(filename):\n",
        "    \"\"\"\n",
        "    Parse the extracted filename to get metadata\n",
        "    Format: zipname_subfolder_filename.peps.txt or zipname_parameters.js\n",
        "    \"\"\"\n",
        "    base = os.path.basename(filename)\n",
        "    parts = base.split('_')\n",
        "\n",
        "    if filename.endswith('parameters.js'):\n",
        "        # This is a parameters file\n",
        "        return {\n",
        "            'zip_name': '_'.join(parts[:-1]),  # Everything before _parameters.js\n",
        "            'file_type': 'parameters',\n",
        "            'filename': base\n",
        "        }\n",
        "    else:\n",
        "        # This is a peps file - try to extract student, sample, and conditions\n",
        "        try:\n",
        "            # Assuming format: StudentInitials-SampleName-Conditions_subfolder_filename.peps.txt\n",
        "            zip_parts = parts[0].split('-')\n",
        "            if len(zip_parts) >= 3:\n",
        "                return {\n",
        "                    'student': zip_parts[0],\n",
        "                    'sample': zip_parts[1],\n",
        "                    'conditions': zip_parts[2],\n",
        "                    'subfolder': parts[1] if len(parts) > 2 else '',\n",
        "                    'file_type': 'peps',\n",
        "                    'filename': base\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    'zip_name': parts[0],\n",
        "                    'subfolder': parts[1] if len(parts) > 2 else '',\n",
        "                    'file_type': 'peps',\n",
        "                    'filename': base\n",
        "                }\n",
        "        except Exception as e:\n",
        "            # If parsing fails, return basic info\n",
        "            return {\n",
        "                'zip_name': parts[0],\n",
        "                'file_type': 'peps' if filename.endswith('.peps.txt') else 'unknown',\n",
        "                'filename': base,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "# Create dataframes to track files and metadata\n",
        "peps_metadata = [parse_extracted_filename(f) for f in extracted_peps_files]\n",
        "params_metadata = [parse_extracted_filename(f) for f in extracted_params_files]\n",
        "\n",
        "peps_df = pd.DataFrame(peps_metadata)\n",
        "params_df = pd.DataFrame(params_metadata)\n",
        "\n",
        "# Display summary of extracted files\n",
        "if not peps_df.empty:\n",
        "    print(\"\\nPeptide files summary:\")\n",
        "    if 'student' in peps_df.columns and 'sample' in peps_df.columns:\n",
        "        student_counts = peps_df['student'].value_counts()\n",
        "        sample_counts = peps_df['sample'].value_counts()\n",
        "        print(f\"Files by student: {dict(student_counts)}\")\n",
        "        print(f\"Files by sample: {dict(sample_counts)}\")\n",
        "    else:\n",
        "        print(\"Could not extract detailed metadata from filenames\")\n",
        "        print(f\"Files by zip source: {dict(peps_df['zip_name'].value_counts())}\")\n",
        "\n",
        "# Save metadata to CSV for reference\n",
        "peps_df.to_csv(os.path.join(extracted_dir, 'peps_files_metadata.csv'), index=False)\n",
        "params_df.to_csv(os.path.join(extracted_dir, 'params_files_metadata.csv'), index=False)"
      ],
      "metadata": {
        "id": "GPA8iA8qkjXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enriching the Data\n",
        "This section loads the combined data file and enhances it by processing the various parameters from the search. The code:\n",
        "1. Loads the previously combined peptide and parameter data\n",
        "2. Processes parameter columns that contain complex data (like lists of PTMs)\n",
        "3. Creates boolean columns for each type of parameter (e.g., each enzyme type, each fixed or variable PTM)\n",
        "4. Extracts raw filename and database filename information\n",
        "5. Parses actual sample types based on raw filenames\n",
        "6. Provides detailed information about the updated dataframe\n",
        "7. Saves the enriched dataframe to a new CSV file\n",
        "\n",
        "\n",
        "This enrichment makes the data easier to analyze because it transforms complex nested data into simple boolean columns.\n",
        "\n"
      ],
      "metadata": {
        "id": "vEFurbyMy_Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the combined data\n",
        "file_path = '/content/combined_peps_parameters.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(\"Initial DataFrame shape (rows, columns):\", df.shape)\n",
        "\n",
        "# Process parameter columns that are currently stored as strings but represent lists or objects\n",
        "params_to_process = [\n",
        "    'spectraFilenames',\n",
        "    'dbFilenames',\n",
        "    'fixedPtms',\n",
        "    'variablePtms'\n",
        "]\n",
        "\n",
        "# Extract values from string representations of lists\n",
        "for param in params_to_process:\n",
        "    if param in df.columns:\n",
        "        # Create new columns for each item in these lists\n",
        "        try:\n",
        "            # First, try to parse the string representations of lists\n",
        "            new_values = {}\n",
        "\n",
        "            for idx, value in df[param].items():\n",
        "                if pd.isna(value):\n",
        "                    continue\n",
        "\n",
        "                # Handle string representations of lists\n",
        "                if isinstance(value, str):\n",
        "                    # Clean up the string to make it properly parseable\n",
        "                    clean_value = value.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
        "\n",
        "                    try:\n",
        "                        # Try to parse as JSON\n",
        "                        parsed = json.loads(clean_value)\n",
        "                    except:\n",
        "                        try:\n",
        "                            # Try to parse as Python literal\n",
        "                            parsed = ast.literal_eval(value)\n",
        "                        except:\n",
        "                            # If all parsing fails, just use the string as is\n",
        "                            parsed = value\n",
        "\n",
        "                    new_values[idx] = parsed\n",
        "                else:\n",
        "                    new_values[idx] = value\n",
        "\n",
        "            # Create a new series with the parsed values\n",
        "            parsed_series = pd.Series(new_values)\n",
        "\n",
        "            # For list columns, create individual columns for each item\n",
        "            if len(parsed_series) > 0 and isinstance(parsed_series.iloc[0], list):\n",
        "                # Get all unique values across all lists\n",
        "                all_items = set()\n",
        "                for idx, items in parsed_series.items():\n",
        "                    if isinstance(items, list):\n",
        "                        all_items.update(items)\n",
        "\n",
        "                # Create a column for each item\n",
        "                for item in sorted(all_items):\n",
        "                    col_name = f\"{param}_{item.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "                    df[col_name] = False\n",
        "\n",
        "                    # Mark True where the item exists in the list\n",
        "                    for idx, items in parsed_series.items():\n",
        "                        if isinstance(items, list) and item in items:\n",
        "                            df.at[idx, col_name] = True\n",
        "\n",
        "            # For scalar values, just keep the original column\n",
        "            else:\n",
        "                df[param] = parsed_series\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {param}: {e}\")\n",
        "\n",
        "# Extract raw filename from spectraFilenames\n",
        "if 'spectraFilenames' in df.columns:\n",
        "    def get_first_raw_file(value):\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if isinstance(value, list) and len(value) > 0:\n",
        "                return value[0]\n",
        "            elif isinstance(value, str):\n",
        "                # Try to parse as a list if it looks like one\n",
        "                if value.startswith('[') and value.endswith(']'):\n",
        "                    parsed = ast.literal_eval(value)\n",
        "                    if isinstance(parsed, list) and len(parsed) > 0:\n",
        "                        return parsed[0]\n",
        "                return value\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    df['raw_filename'] = df['spectraFilenames'].apply(get_first_raw_file)\n",
        "\n",
        "# Extract database filename from dbFilenames\n",
        "if 'dbFilenames' in df.columns:\n",
        "    def get_first_db_file(value):\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if isinstance(value, list) and len(value) > 0:\n",
        "                return value[0]\n",
        "            elif isinstance(value, str):\n",
        "                # Try to parse as a list if it looks like one\n",
        "                if value.startswith('[') and value.endswith(']'):\n",
        "                    parsed = ast.literal_eval(value)\n",
        "                    if isinstance(parsed, list) and len(parsed) > 0:\n",
        "                        return parsed[0]\n",
        "                return value\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    df['db_filename'] = df['dbFilenames'].apply(get_first_db_file)\n",
        "\n",
        "# Extract enzyme information\n",
        "if 'enzymeSelection' in df.columns:\n",
        "    # Create boolean columns for each enzyme type\n",
        "    unique_enzymes = df['enzymeSelection'].unique()\n",
        "    for enzyme in unique_enzymes:\n",
        "        if pd.notna(enzyme):\n",
        "            col_name = f\"enzyme_{enzyme}\"\n",
        "            df[col_name] = df['enzymeSelection'] == enzyme\n",
        "\n",
        "# Extract PTM information into separate columns\n",
        "def create_ptm_columns(row):\n",
        "    \"\"\"Create boolean columns for each PTM type\"\"\"\n",
        "    result = {}\n",
        "\n",
        "    # Process fixed PTMs\n",
        "    if 'fixedPtms' in df.columns:\n",
        "        fixed_ptms = row['fixedPtms']\n",
        "        if isinstance(fixed_ptms, list):\n",
        "            for ptm in fixed_ptms:\n",
        "                result[f\"fixed_ptm_{ptm.replace(' ', '_')}\"] = True\n",
        "\n",
        "    # Process variable PTMs\n",
        "    if 'variablePtms' in df.columns:\n",
        "        var_ptms = row['variablePtms']\n",
        "        if isinstance(var_ptms, list):\n",
        "            for ptm in var_ptms:\n",
        "                result[f\"var_ptm_{ptm.replace(' ', '_')}\"] = True\n",
        "\n",
        "    return result\n",
        "\n",
        "# Apply the function and add the columns\n",
        "if 'fixedPtms' in df.columns or 'variablePtms' in df.columns:\n",
        "    # First create all possible PTM columns\n",
        "    all_fixed_ptms = set()\n",
        "    all_var_ptms = set()\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Fixed PTMs\n",
        "        if 'fixedPtms' in df.columns:\n",
        "            fixed_ptms = row['fixedPtms']\n",
        "            if isinstance(fixed_ptms, list):\n",
        "                all_fixed_ptms.update(fixed_ptms)\n",
        "\n",
        "        # Variable PTMs\n",
        "        if 'variablePtms' in df.columns:\n",
        "            var_ptms = row['variablePtms']\n",
        "            if isinstance(var_ptms, list):\n",
        "                all_var_ptms.update(var_ptms)\n",
        "\n",
        "    # Create columns for all PTMs\n",
        "    for ptm in all_fixed_ptms:\n",
        "        col_name = f\"fixed_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "        df[col_name] = False\n",
        "\n",
        "    for ptm in all_var_ptms:\n",
        "        col_name = f\"var_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "        df[col_name] = False\n",
        "\n",
        "    # Now fill in the values\n",
        "    for idx, row in df.iterrows():\n",
        "        # Fixed PTMs\n",
        "        if 'fixedPtms' in df.columns:\n",
        "            fixed_ptms = row['fixedPtms']\n",
        "            if isinstance(fixed_ptms, list):\n",
        "                for ptm in fixed_ptms:\n",
        "                    col_name = f\"fixed_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "                    df.at[idx, col_name] = True\n",
        "\n",
        "        # Variable PTMs\n",
        "        if 'variablePtms' in df.columns:\n",
        "            var_ptms = row['variablePtms']\n",
        "            if isinstance(var_ptms, list):\n",
        "                for ptm in var_ptms:\n",
        "                    col_name = f\"var_ptm_{ptm.replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
        "                    df.at[idx, col_name] = True\n",
        "\n",
        "# Extract sample from raw filename\n",
        "def extract_sample_from_raw(raw_filename):\n",
        "    if pd.isna(raw_filename):\n",
        "        return \"unknown\"\n",
        "\n",
        "    raw_str = str(raw_filename).lower()\n",
        "\n",
        "    if 'bovine' in raw_str or 'cow' in raw_str:\n",
        "        return \"cow\"\n",
        "    elif 'mammoth' in raw_str:\n",
        "        return \"mammoth\"\n",
        "    elif 'edmontosaur' in raw_str:\n",
        "        return \"edmontosaurus\"\n",
        "    elif 'turkey' in raw_str:\n",
        "        return \"turkey\"\n",
        "    else:\n",
        "        return raw_str.split('.')[0]\n",
        "\n",
        "# Add actual_sample column based on raw filename\n",
        "if 'raw_filename' in df.columns:\n",
        "    df['actual_sample'] = df['raw_filename'].apply(extract_sample_from_raw)\n",
        "\n",
        "# Print information about the updated dataframe\n",
        "print(\"\\nUpdated DataFrame shape (rows, columns):\", df.shape)\n",
        "print(\"\\nNew columns added:\")\n",
        "new_cols = set(df.columns) - set(['line', 'target/decoy', 'nDecoy', 'specId', 'scanNum', 'mz', 'z', 'ppm', 'score',\n",
        "                             'peptide', 'protein', 'student', 'sample', 'conditions', 'original_filename', 'notes',\n",
        "                             'jobTitle', 'spectraFilenames', 'dbFilenames', 'fixedPtms', 'variablePtms',\n",
        "                             'enzymeSelection', 'fragmentationSelection', 'massAnalyzer', 'precursorErrorTol',\n",
        "                             'precursorErrorTolUnit', 'fragmentErrorTol', 'fragmentErrorTolUnit',\n",
        "                             'activateBuiltInPtms', 'peps_file', 'param_file', 'condition'])\n",
        "for col in sorted(new_cols):\n",
        "    print(f\"  {col}\")\n",
        "\n",
        "# Show sample distribution\n",
        "if 'actual_sample' in df.columns:\n",
        "    print(\"\\nSample distribution:\")\n",
        "    sample_counts = df['actual_sample'].value_counts()\n",
        "    for sample, count in sample_counts.items():\n",
        "        print(f\"  {sample}: {count} entries\")\n",
        "\n",
        "# Find unique combinations of samples and methods\n",
        "if 'actual_sample' in df.columns:\n",
        "    # Use database file as a proxy for search strategy\n",
        "    if 'db_filename' in df.columns:\n",
        "        print(\"\\nUnique sample-database combinations:\")\n",
        "        combinations = df.groupby(['actual_sample', 'db_filename']).size().reset_index(name='count')\n",
        "        for _, row in combinations.iterrows():\n",
        "            print(f\"  {row['actual_sample']} with {row['db_filename']}: {row['count']} entries\")\n",
        "\n",
        "        # Find samples analyzed with multiple databases\n",
        "        sample_db_counts = df.groupby('actual_sample')['db_filename'].nunique()\n",
        "        samples_with_multiple_dbs = sample_db_counts[sample_db_counts > 1].index.tolist()\n",
        "\n",
        "        print(f\"\\nSamples analyzed with multiple databases: {samples_with_multiple_dbs}\")\n",
        "\n",
        "        # For each such sample, show the databases\n",
        "        for sample in samples_with_multiple_dbs:\n",
        "            dbs = sorted(df[df['actual_sample'] == sample]['db_filename'].unique())\n",
        "            print(f\"  {sample}:\")\n",
        "            for db in dbs:\n",
        "                count = len(df[(df['actual_sample'] == sample) & (df['db_filename'] == db)])\n",
        "                print(f\"    {db}: {count} entries\")\n",
        "\n",
        "        # Now, for each sample with multiple databases, compare the spectrum identifications\n",
        "        for sample in samples_with_multiple_dbs:\n",
        "            print(f\"\\n===== COMPARING DATABASES FOR {sample.upper()} =====\")\n",
        "\n",
        "            # Get data for this sample\n",
        "            sample_data = df[df['actual_sample'] == sample]\n",
        "\n",
        "            # Get unique databases for this sample\n",
        "            dbs = sorted(sample_data['db_filename'].unique())\n",
        "\n",
        "            # Get spectra identified under each database\n",
        "            db_spectra = {}\n",
        "            for db in dbs:\n",
        "                db_df = sample_data[sample_data['db_filename'] == db]\n",
        "                spectra = set(db_df['specId'].astype(str))\n",
        "                db_spectra[db] = spectra\n",
        "                print(f\"  {db}: {len(spectra)} spectra identified\")\n",
        "\n",
        "            # Compare first two databases (for simplicity)\n",
        "            if len(dbs) >= 2:\n",
        "                db1, db2 = dbs[0], dbs[1]\n",
        "\n",
        "                # Calculate overlap\n",
        "                overlap = db_spectra[db1].intersection(db_spectra[db2])\n",
        "                only_db1 = db_spectra[db1] - db_spectra[db2]\n",
        "                only_db2 = db_spectra[db2] - db_spectra[db1]\n",
        "\n",
        "                # Calculate overlap percentages\n",
        "                overlap_pct_of_db1 = len(overlap) / len(db_spectra[db1]) * 100 if db_spectra[db1] else 0\n",
        "                overlap_pct_of_db2 = len(overlap) / len(db_spectra[db2]) * 100 if db_spectra[db2] else 0\n",
        "\n",
        "                print(f\"\\n  {db1} vs {db2}:\")\n",
        "                print(f\"    Overlap: {len(overlap)} spectra ({overlap_pct_of_db1:.1f}% of {db1}, {overlap_pct_of_db2:.1f}% of {db2})\")\n",
        "                print(f\"    Only in {db1}: {len(only_db1)} spectra\")\n",
        "                print(f\"    Only in {db2}: {len(only_db2)} spectra\")\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df.to_csv('/content/enriched_peps_parameters.csv', index=False)\n",
        "print(\"\\nEnriched DataFrame saved to: /content/enriched_peps_parameters.csv\")\n"
      ],
      "metadata": {
        "id": "htbhElkoztUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "This section provides a comprehensive overview of the dataframe structure and contents. The code:\n",
        "1. Shows the overall shape of the dataframe (number of rows and columns)\n",
        "2. Lists all column names with their indices for easy reference\n",
        "3. Displays the data types of each column\n",
        "4. Shows the first 5 rows of the dataframe\n",
        "\n",
        "This helps you understand what data you're working with and how it's structured before you begin your analysis."
      ],
      "metadata": {
        "id": "_KuTe2SMz3Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about the dataframe\n",
        "print(\"DataFrame shape (rows, columns):\", df.shape)\n",
        "\n",
        "# Display column names\n",
        "print(\"\\nColumn names:\")\n",
        "for i, col in enumerate(df.columns):\n",
        "    print(f\"{i+1}. {col}\")\n",
        "\n",
        "# Display data types of each column\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# First 5 rows\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "tmicDsIJ0AnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps for Analysis\n",
        "Now that the data is prepared and explored, you can add additional analysis cells to:\n",
        "\n",
        "1. **Filter peptides by sample type**:"
      ],
      "metadata": {
        "id": "0YvQ05s80Ib3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "   # Example: Select only peptides from the mammoth sample\n",
        "   mammoth_peptides = df[df['actual_sample'] == 'mammoth']\n",
        "   print(f\"Found {len(mammoth_peptides)} mammoth peptides\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d1X_wlw0Ogo",
        "outputId": "b47becc6-f720-4b79-f4d9-496938f0efe1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 50457 mammoth peptides\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TfH3nYxQ0Te5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Analyze peptide modifications**:"
      ],
      "metadata": {
        "id": "bTyepnak0Qqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Count peptides with hydroxyproline modifications\n",
        "hyp_cols = [col for col in df.columns if 'hydroxy' in col.lower() and col.startswith('variable')]\n",
        "has_hyp = df[hyp_cols].any(axis=1)\n",
        "print(f\"Found {has_hyp.sum()} peptides with hydroxyproline modifications\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M-x4CYD0Xsr",
        "outputId": "51aa83a8-5f08-4f4a-a481-bf3eb41965ee"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 49661 peptides with hydroxyproline modifications\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Compare identification efficiency across samples**:"
      ],
      "metadata": {
        "id": "VhWQeD940iuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compare how many unique peptides were identified per sample\n",
        "peptides_by_sample = df.groupby('actual_sample')['peptide'].nunique()\n",
        "print(\"Unique peptides by sample:\")\n",
        "print(peptides_by_sample)"
      ],
      "metadata": {
        "id": "j1w1_s500nen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Visualize score distributions**"
      ],
      "metadata": {
        "id": "KW8UymGF0r4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Create a histogram of peptide scores by sample\n",
        "plt.figure(figsize=(10, 6))\n",
        "for sample in df['actual_sample'].unique():\n",
        "  sample_data = df[df['actual_sample'] == sample]\n",
        "  plt.hist(sample_data['score'], alpha=0.5, label=sample, bins=20)\n",
        "plt.legend()\n",
        "plt.title('Distribution of Peptide Scores by Sample')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l58kSyYazbGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}